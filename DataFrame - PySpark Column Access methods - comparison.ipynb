{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f84da482-3084-4ef8-b5d0-053daeebfddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#PySpark Dataframes - compare various methods to access columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a057c8f-aac1-4ba3-a2ba-7ac880a52501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##PySpark Dataframe: Available methods to access columns\n",
    "\n",
    "* Method 1: **Dot notation** \n",
    "    \n",
    "    e.g. `df_nation.select(df_nation.n_name, df_nation.n_regionkey)`\n",
    "* Method 2: **Square bracket notation**\n",
    "    \n",
    "    e.g. `df_nation.select(df_nation[\"n_name\"], df_nation[\"n_regionkey\"])`\n",
    "* Method 3: **Using col() / column() functions**\n",
    "\n",
    "    Using col(): `df_nation.select(col(\"n_name\"), col(\"n_regionkey\"))`\n",
    "\n",
    "    Using column(): `df_nation.select(column(\"n_name\"), column(\"n_regionkey\"))`\n",
    "\n",
    "* Method 4: **Specifying column name as string literals**\n",
    "\n",
    "    Using double quotes: `df_nation.select(\"n_name\", \"n_regionkey\")`\n",
    "\n",
    "    Using single quotes: `df_nation.select('n_name', 'n_regionkey')`\n",
    "\n",
    "* Method 5: **Using expr() / selectExpr() functions**\n",
    "\n",
    "    Using expr(): `df_nation.select(\"n_name\", expr(\"n_regionkey\"), expr(\"concat_ws(':', n_name, n_regionkey) as n_region\"))`\n",
    "\n",
    "    Using selectExpr(): `df_nation.selectExpr(\"n_name\", \"n_regionkey\", \"concat_ws(':', n_name, n_regionkey) as n_region\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df579bbc-36da-4c8e-852b-4105e9ab49b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Sample data - part 1\n",
    "\n",
    "Note: The sample tables as below are part of the samples catalog that comes along with the Free Edition of Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd49c00-9a85-4544-aef1-f1d272cb8fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#25\n",
    "df_nation = spark.read.table(\"samples.tpch.nation\")\n",
    "display(df_nation.limit(100))\n",
    "\n",
    "#750,000\n",
    "df_customer = spark.read.table(\"samples.tpch.customer\")\n",
    "display(df_customer.limit(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cefdea08-c65e-4e21-8478-49e456daa681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Scenarios considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0269a1f7-08e9-4163-8e6f-dbb66203c1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 1: SELECT statements using column names that do not contain special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83e1829-b9dd-4eb4-babc-490a78c65574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Method 1: Dot notation\n",
    "df_result = df_nation.select(df_nation.n_name, df_nation.n_regionkey)\n",
    "df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "df_result = df_nation.select(df_nation[\"n_name\"], df_nation[\"n_regionkey\"])\n",
    "df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "from pyspark.sql.functions import col, column\n",
    "#3.1 Using col()\n",
    "df_result = df_nation.select(col(\"n_name\"), col(\"n_regionkey\"))\n",
    "df_result.display()\n",
    "#3.2 Using column()\n",
    "df_result = df_nation.select(column(\"n_name\"), column(\"n_regionkey\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "#4.1 Column name in double quotes\n",
    "df_result = df_nation.select(\"n_name\", \"n_regionkey\")\n",
    "df_result.display()\n",
    "#4.2 Column name in single quotes\n",
    "df_result = df_nation.select('n_name', 'n_regionkey')\n",
    "df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "from pyspark.sql.functions import expr\n",
    "#5.1 Using expr()\n",
    "df_result = df_nation.select(\"n_name\", expr(\"n_regionkey\"), expr(\"concat_ws(':', n_name, n_regionkey) as n_region\"))\n",
    "df_result.display()\n",
    "#5.2 Using selectExpr()\n",
    "df_result = df_nation.selectExpr(\"n_name\", \"n_regionkey\", \"concat_ws(':', n_name, n_regionkey) as n_region\")\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb9dc1c6-0b04-473b-ac3a-619b8bee633c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Notes**: All the five methods work fine, as the column names didn't contain any special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d42f129-1d84-4657-b49f-7e38d4290890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 2: SELECT statements using column names that contain special characters\n",
    "\n",
    "Example special characters: space, hyphen (-) , @ etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ad5e73-d3dd-4982-8325-611c6c75cc51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sample dataframe with column names that contain special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca9113b-f2b5-4d29-b52a-1dcfc45544b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampleData = (\n",
    "    (\"Kevin\",\"\",\"S\",\"NewYork\",3100), \n",
    "    (\"David\",\"R\",\"\",\"California\",4300), \n",
    "    (\"Ben\",\"L\",\"J\",\"NewYork\",3000) \n",
    "  )\n",
    "\n",
    "#Notice special characters in column names: space, @, - etc.\n",
    "myschema = (\n",
    "    \"`first name` string, \"\n",
    "    \"`middle@name` string, \"\n",
    "    \"`last-name` string, \"\n",
    "    \"location string, \"\n",
    "    \"salary int\"\n",
    ")\n",
    "\n",
    "df_specialCharacters = spark.createDataFrame(data = sampleData, schema = myschema)\n",
    "\n",
    "df_specialCharacters.select(\"*\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4ac1656-74f8-43ec-9423-abd63fc1355d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756360061838}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Method 1: Dot notation\n",
    "#ATTENTION: FAILS as it doesn't support special characters in column names.\n",
    "# df_results_sc = df_specialCharacters.select(df_specialCharacters.first name, df_specialCharacters.middle@name)\n",
    "# df_results_sc.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "df_results_sc = df_specialCharacters.select(df_specialCharacters[\"first name\"], df_specialCharacters[\"middle@name\"])\n",
    "df_results_sc.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "from pyspark.sql.functions import col, column\n",
    "df_results_sc = df_specialCharacters.select(col(\"first name\"), col(\"middle@name\"))\n",
    "df_results_sc.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "#4.1 Column name in double quotes\n",
    "df_results_sc = df_specialCharacters.select(\"first name\", \"middle@name\")\n",
    "df_results_sc.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "#IMPORTANT: Use backticks to enclose column names that have special characters in it.\n",
    "from pyspark.sql.functions import expr\n",
    "#5.1 Using expr()\n",
    "df_results_sc = df_specialCharacters.select(expr(\"`first name`\"), expr(\"`middle@name`\"), expr(\"`last-name`\"), expr(\"concat_ws(':', `first name`, `middle@name`, `last-name`) as full_name\"))\n",
    "df_results_sc.display()\n",
    "#5.2 Using selectExpr()\n",
    "df_results_sc = df_specialCharacters.selectExpr(\"`first name`\", \"`middle@name`\", \"`last-name`\", \"concat_ws(':', `first name`, `middle@name`, `last-name`) as full_name\")\n",
    "df_results_sc.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5db44815-12c5-4098-bf05-9cc5344727e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes:\n",
    "\n",
    "| Method | supports special characters? |\n",
    "|--------|------------------------------|\n",
    "|Dot notation|No. Doesn't support special characters in column names|\n",
    "|Square bracket notation|Yes|\n",
    "|col() function|Yes|\n",
    "|column name as string literals|Yes|\n",
    "|expr() / selectExpr()|Yes, provided backticks are used to enclose the column names|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "370de890-88b9-4f0b-82db-66c378f987b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 3: Is it able to return column object?\n",
    "\n",
    "Tip: The benefits in returning a column object are that we can invoke various methods on the column object.\n",
    "\n",
    "Example methods available on a column object:\n",
    "\n",
    "* alias()\n",
    "* asc()\n",
    "* between()\n",
    "* cast()\n",
    "* contains()\n",
    "* desc()\n",
    "* endswith()\n",
    "* ilike()\n",
    "* isNull()\n",
    "* substr() etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c57b5950-0147-4a63-8edb-615bdcc4aa6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Method 1: Dot notation - invoking alias(), cast() methods on column object\n",
    "df_result = df_nation.select(df_nation.n_name.alias(\"nation@name\"))\n",
    "df_result.display()\n",
    "df_result = df_nation.select(df_nation.n_nationkey.cast(\"string\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation - invoking alias(), cast() methods on column object\n",
    "df_result = df_nation.select(df_nation[\"n_name\"].alias(\"nation@name\"))\n",
    "df_result.display()\n",
    "df_result = df_nation.select(df_nation[\"n_nationkey\"].cast(\"string\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function - invoking alias(), cast() methods on column object\n",
    "from pyspark.sql.functions import col, column\n",
    "df_result = df_nation.select(col(\"n_name\").alias(\"nation@name\"))\n",
    "df_result.display()\n",
    "df_result = df_nation.select(col(\"n_nationkey\").cast(\"string\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals - invoking alias(), cast() methods on column object\n",
    "#ATTENTION: Fails. Because, column name as a string literal doesn't return column object. Instead, it returns a string object. \n",
    "#df_result = df_nation.select(\"n_name\".alias(\"nation@name\"))\n",
    "#df_result.display()\n",
    "#df_result = df_nation.select(\"n_nationkey\".cast(\"string\"))\n",
    "#df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function - invoking alias(), cast() methods on column object\n",
    "from pyspark.sql.functions import expr\n",
    "df_result = df_nation.select(expr(\"n_name\").alias(\"nation@name\"))\n",
    "df_result.display()\n",
    "df_result = df_nation.select(expr(\"n_nationkey\").cast(\"string\"))\n",
    "df_result.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74961720-032e-455a-8380-e9b0125bdce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes:\n",
    "* With the exception of column name as string literal method, all other methods return a column object. \n",
    "\n",
    "| Method | return column object? |\n",
    "|--------|------------------------------|\n",
    "|Dot notation|Yes|\n",
    "|Square bracket notation|Yes|\n",
    "|col() function|Yes|\n",
    "|column name as string literals|No. Instead of column object, it returns a string object|\n",
    "|expr() / selectExpr()|Yes|\n",
    "\n",
    "* So, when you use a method that returns a column object, you could invoke all the available column methods on such column object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66773cd0-277c-4ee4-8e5a-a0902d865232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 4: Ability to apply column operators (i.e. Column Expressions)\n",
    "\n",
    "Example column operators: \\ , * , + , < , >= , ==, != \n",
    "\n",
    "To be able to apply column operators, the method we use to access the column must return a column object rather than a string object. \n",
    "\n",
    "If we are able to apply column operators on existing columns in a dataframe, we will be able to construct new columns based on the existing columns.\n",
    "\n",
    "For example: `(df_customer.c_acctbal * 2)` is a column expression, wherein multiply (i.e. * ) operator was used to double the existing column value. Based on this column expression, we can construct a new column in the dataframe.\n",
    "\n",
    "Note: Column expressions aren't the same as `expr()` / `selectExpr()`. Column expression is defined based on a column object as returned by dot notation/square bracket notation/col(). Whereas, `expr()` / `selectExpr()` are based on a string literal that contains within it as a SQL-like expression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8f56605-4846-48b8-96e3-6592df693cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Example operator: using * (i.e. multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0655ee5-cfc8-47ac-a37b-52166f86d3e9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756539285278}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Method 1: Dot notation\n",
    "df_result = df_customer.select((df_customer.c_acctbal * 2).alias(\"acctbalDoubled\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "df_result = df_customer.select((df_customer[\"c_acctbal\"] * 2).alias(\"acctbalDoubled\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "from pyspark.sql.functions import col\n",
    "df_result = df_customer.select((col(\"c_acctbal\") * 2).alias(\"acctbalDoubled\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "#ATTENTION: Fails because column name as string literal doesn't return column object. Instead it returns string object. So, you cannot apply operators. \n",
    "# df_result = df_customer.select(((\"c_acctbal\") * 2).alias(\"acctbalDoubled\"))\n",
    "# df_result.display()\n",
    "#Notes: unlike column object's methods, string object's methods work on the string literal itself rather than on the returned result.\n",
    "#For example, if you invoke the upper() method on a column object, it returns a column object with the upper() method applied to the results. On the other hand, if you invoke the upper() method on a string literal, it returns the column name in uppercase rather than coverting the results to upper case.\n",
    "#df_result = df_customer.select(\"(c_comment\").upper())\n",
    "#df_result.display()\n",
    "#df_result = df_customer.select(\"c_comment\".upper())\n",
    "#df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "from pyspark.sql.functions import expr\n",
    "df_result = df_customer.select(\"c_acctbal\", (expr(\"c_acctbal\") * 2).alias(\"acctbalDoubled\"))\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dbe14c9-8f23-460b-8db9-70bddef0640e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes:\n",
    "\n",
    "* Similar to column methods, even the column operators can be applied only on column objects and not on string literals. String literal is a string object.\n",
    "* For example, if you invoke the upper() method on a column object, it returns a column object with the upper() method applied to the results. However, if you invoke the upper() method on a string literal, it returns the column name in uppercase rather than upper casing the values in the c_comment column. e.g. (\"c_comment\").upper() would return the column name as C_COMMENT ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09aba0e1-06a3-4b6e-9b0e-52a10529a24f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 5: FILTER / WHERE clause using example operator ( == )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "655330af-6844-4858-8073-6dab52af7bd2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756539707690}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Method 1: Dot notation\n",
    "df_result = df_customer.select(df_customer.c_acctbal).where(df_customer.c_acctbal <= 5000)\n",
    "df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "df_result = df_customer.select(df_customer[\"c_acctbal\"]).where(df_customer[\"c_acctbal\"] <= 5000)\n",
    "df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "from pyspark.sql.functions import col\n",
    "df_result = df_customer.select(col(\"c_acctbal\")).where(col(\"c_acctbal\") <= 5000)\n",
    "df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "#ATTENTION:Fails because column name as string literal doesn't return column object. Instead it returns string object. So, you cannot apply operators. \n",
    "# df_result = df_customer.select(\"c_acctbal\").where((\"c_acctbal\") <= 5000)\n",
    "# df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "from pyspark.sql.functions import expr\n",
    "df_result = df_customer.select(\"c_acctbal\").where ((expr(\"c_acctbal\")) <= 5000)\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcd192fb-1b73-4b49-96cc-d7818242b689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes:\n",
    "\n",
    "* To be able to reference a column in a FILTER / WHERE clause, the method that we use to access the column must return a column object rather than a string object. Only then we can apply operators on such column object in FILTER / WHERE clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02172674-d260-40ad-b723-4d1775e0976d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 6: ORDER BY clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d711c9d2-fe9d-4658-af56-f0e6c7e4361d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756540134932}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Method 1: Dot notation\n",
    "df_result = df_customer.select(\"*\").where(df_customer.c_acctbal <= 5000).orderBy(df_customer.c_acctbal.desc())\n",
    "df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "df_result = df_customer.select(\"*\").where(df_customer[\"c_acctbal\"] <= 5000).orderBy(df_customer[\"c_acctbal\"].desc())\n",
    "df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "from pyspark.sql.functions import col\n",
    "df_result = df_customer.select(\"*\").where(col(\"c_acctbal\") <= 5000).orderBy(col(\"c_acctbal\").desc())\n",
    "df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "#Note: Notice how we are able to specify string literal as column name in the orderBy clause, by also specifying with it \"ascending\" param value.\n",
    "df_result = df_customer.select(\"*\").where(col(\"c_acctbal\") <= 5000).orderBy(\"c_acctbal\", ascending=False)\n",
    "df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "from pyspark.sql.functions import expr\n",
    "df_result = df_customer.select(\"c_acctbal\", (expr(\"c_acctbal\") * 2).alias(\"acctbalDoubled\")).where(expr(\"c_acctbal\") <= 5000).orderBy(expr(\"c_acctbal\").desc())\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca56fc1e-1acd-4365-a765-7ab5b3356dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes:\n",
    "\n",
    "Unlike FILTER / WHERE clause, the ORDER BY clause supports all the five methods of accessing columns. However, in case of column name as string literal in orderBy clause, the ascending/descending parameter value will need to be specified separately as 2nd argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bce174c-b5a0-4f64-9bcf-d98771943891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 7: Accessing aliased columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c59894a-c9ce-49b9-8ebe-d77ce7179b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Method 1: Dot notation\n",
    "#ATTENTION: Fails as Dot notation doesn't support accessing aliased column.\n",
    "# df_result = df_customer.select((df_customer.c_acctbal * 2).alias(\"acctbalDoubled\")).where(df_customer.acctbalDoubled <= 10000)\n",
    "# df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "#ATTENTION: Fails as Dot notation doesn't support accessing aliased column.\n",
    "# df_result = df_customer.select((df_customer.c_acctbal * 2).alias(\"acctbalDoubled\")).where(df_customer[\"acctbalDoubled\"] <= 10000)\n",
    "# df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "from pyspark.sql.functions import col\n",
    "df_result = df_customer.select((df_customer.c_acctbal * 2).alias(\"acctbalDoubled\")).where(col(\"acctbalDoubled\") <= 10000)\n",
    "df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "#ATTENTION: Fails as Dot notation doesn't support accessing aliased column.\n",
    "# df_result = df_customer.select((df_customer.c_acctbal * 2).alias(\"acctbalDoubled\")).where(\"acctbalDoubled\" <= 10000)\n",
    "# df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "from pyspark.sql.functions import expr\n",
    "df_result = df_customer.select((df_customer.c_acctbal * 2).alias(\"acctbalDoubled\")).where(expr(\"acctbalDoubled\") <= 10000)\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5441730-1be1-427a-a7e5-c9b24fa4a050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes:\n",
    "\n",
    "* An aliased column can be subsequently accessed using `col()` and `expr()` methods only. We cannot use string literal option to access aliased column name. Similarly, we cannot use dot notation and square bracket notation to access aliased column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3be682e7-a877-4ebd-be05-8a6204ca7032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 8: Adding column / renaming column in a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb240471-6cfa-409a-9969-f2b31bff769e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####8.1 withColumn() method of Dataframe\n",
    "\n",
    "Syntax: `DataFrame.withColumn(colName, col)`\n",
    "\n",
    "Parameters to the `withColumn` method are:\n",
    "* colName: string, name of the new column.\n",
    "* col: column object or column expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a36678-78b9-4ab6-9f46-0fb9cc88962f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Method 1: Dot notation\n",
    "df_result = df_nation.select(df_nation.n_name, df_nation.n_regionkey).withColumn(\"nation@name\", df_nation.n_name)\n",
    "df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "df_result = df_nation.select(df_nation[\"n_name\"], df_nation[\"n_regionkey\"]).withColumn(\"nation@name\", df_nation[\"n_name\"])\n",
    "df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "from pyspark.sql.functions import col, column\n",
    "#3.1 Using col()\n",
    "df_result = df_nation.select(col(\"n_name\"), col(\"n_regionkey\")).withColumn(\"nation@name\", col(\"n_name\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "#ATTENTION: Fails. Because, the 2nd param to the withColumn() is expected to be a column object/expression and not a string literal\n",
    "#4.1 Column name in double quotes\n",
    "# df_result = df_nation.select(\"n_name\", \"n_regionkey\").withColumn(\"nation@name\", \"n_name\")\n",
    "# df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "from pyspark.sql.functions import expr\n",
    "#5.1 Using expr()\n",
    "df_result = df_nation.select(\"n_name\", \"n_regionkey\").withColumn(\"nation@name\", expr(\"n_name\"))\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb591fd-2832-4bdf-8107-fce9b2d2dafd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes: To the `withColumn` method of Dataframe:\n",
    "* The first param (i.e. new column name) must ALWAYS be a string literal only. It can contain special characters.\n",
    "* The second param must ALWAYS be a column object or column expression.\n",
    "* All the four methods as below return a column object\n",
    "  - dot notation\n",
    "  - square bracket notation\n",
    "  - col() / column ()\n",
    "  - expr()\n",
    "* However, as 2nd param, column name as string literals is not acceptable. Because, it doesn't return a column object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2f0af33-b415-454a-ba6c-282485073628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####8.2 withColumns() method of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2016614b-b999-4a8d-ad32-a2510c65d63c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Method 1: Dot notation\n",
    "df_result = df_nation.select(df_nation.n_name, df_nation.n_regionkey).withColumns({\"nation@name\": df_nation.n_name, \"region-key\": df_nation.n_regionkey})\n",
    "df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "df_result = df_nation.select(df_nation[\"n_name\"], df_nation[\"n_regionkey\"]).withColumns({\"nation@name\": df_nation[\"n_name\"], \"region-key\": df_nation[\"n_regionkey\"]})\n",
    "df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "from pyspark.sql.functions import col, column\n",
    "#3.1 Using col()\n",
    "df_result = df_nation.select(col(\"n_name\"), col(\"n_regionkey\")).withColumns({\"nation@name\": col(\"n_name\"), \"region-key\": col(\"n_regionkey\")})\n",
    "df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "#ATTENTION: Fails. Because, in each key-value pair in the dictionary as param to the withColumns(), the value is expected to be a column object/expression and not a string literal\n",
    "#4.1 Column name in double quotes\n",
    "# df_result = df_nation.select(\"n_name\", \"n_regionkey\").withColumns({\"nation@name\": \"n_name\", \"region-key\": \"n_regionkey\"})\n",
    "# df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "from pyspark.sql.functions import expr\n",
    "#5.1 Using selectExpr()\n",
    "df_result = df_nation.select(\"n_name\", \"n_regionkey\").withColumns({\"nation@name\": expr(\"n_name\"), \"region-key\": expr(\"n_regionkey\")})\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a33d950-0ad5-44ac-8533-3a18f76837f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes: While passing dictionary as param to the `withColumns()` method, each key is expected to be a string literal while the corresponding value is expected to be a column object/expression and not a string literal.\n",
    "\n",
    "The rest of the observations are very similar to that of `withColumn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebb1948c-03d1-4b1c-93bc-c3bbd679867d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####8.3 withColumnRenamed() method of Dataframe\n",
    "\n",
    "Syntax: `DataFrame.withColumnRenamed(existing, new)`\n",
    "\n",
    "Parameters to the `withColumn` method are:\n",
    "* existing: string, The name of the existing column to be renamed.\n",
    "* new: string, The new name to be assigned to the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2bb5b08-3c22-4f8a-91ba-2b1b4ead74bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Method 1: Dot notation\n",
    "#ATTENTION: FAILS, unless both the params to the withColumnRenamed() method were string literals.\n",
    "# df_result = df_nation.select(df_nation.n_name, df_nation.n_regionkey).withColumnRenamed(df_nation.n_name, \"nation@name\")\n",
    "# df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "#ATTENTION: FAILS, unless both the params to the withColumnRenamed() method were string literals.\n",
    "# df_result = df_nation.select(df_nation[\"n_name\"], df_nation[\"n_regionkey\"]).withColumnRenamed(df_nation[\"n_name\"], \"nation@name\")\n",
    "# df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "#ATTENTION: FAILS, unless both the params to the withColumnRenamed() method were string literals.\n",
    "# from pyspark.sql.functions import col, column\n",
    "# #3.1 Using col()\n",
    "# df_result = df_nation.select(col(\"n_name\"), col(\"n_regionkey\")).withColumnRenamed(col(\"n_name\"), \"nation@name\")\n",
    "# df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "#4.1 Column name in double quotes\n",
    "df_result = df_nation.select(\"n_name\", \"n_regionkey\").withColumnRenamed(\"n_name\", \"nation@name\")\n",
    "df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "#ATTENTION: FAILS, unless both the params to the withColumnRenamed() method were string literals.\n",
    "# from pyspark.sql.functions import expr\n",
    "# #5.1 Using expr()\n",
    "# df_result = df_nation.select(\"n_name\", \"n_regionkey\").withColumnRenamed(expr(\"n_name\"), \"nation@name\")\n",
    "# df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ceabbde-f050-4ce2-9b80-74f7fab3a7c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes:\n",
    "\n",
    "To `withColumnRenamed()` method of Dataframe, both column names (existing and new) as params must be string literals only and no column object acceptable.\n",
    "\n",
    "Tip: In case of `withColumn()`, new column name is the first param. Whereas, in case of `withColumnRenamed()`, new column name is the 2nd param. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17d59544-7f24-4384-9fb7-92483393283c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 9: - CASE WHEN statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f9f5d2-3450-4be1-b8f4-e0801df56091",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756448272979}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, expr\n",
    "\n",
    "#Method 1: Dot notation\n",
    "df_result = df_customer.select(df_customer.c_acctbal, \n",
    "  when(df_customer.c_acctbal <= 3000, \"Class A\")\n",
    "  .when(df_customer.c_acctbal <= 6000, \"Class B\")\n",
    "  .otherwise(\"Class C\")\n",
    "  .alias(\"CustomerClassification\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "df_result = df_customer.select(df_customer.c_acctbal, \n",
    "  when(df_customer[\"c_acctbal\"] <= 3000, \"Class A\")\n",
    "  .when(df_customer[\"c_acctbal\"] <= 6000, \"Class B\")\n",
    "  .otherwise(\"Class C\")\n",
    "  .alias(\"CustomerClassification\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "df_result = df_customer.select(df_customer.c_acctbal, \n",
    "  when(col(\"c_acctbal\") <= 3000, \"Class A\")\n",
    "  .when(col(\"c_acctbal\") <= 6000, \"Class B\")\n",
    "  .otherwise(\"Class C\")\n",
    "  .alias(\"CustomerClassification\"))\n",
    "df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "#ATTENTION: Fails. Because column name as string literal wouldn't return column object. As a consequence, it cannot be used along with the column operators such as <= etc. TypeError: '<=' not supported between instances of 'str' and 'int'\n",
    "# df_result = df_customer.select(df_customer.c_acctbal, \n",
    "#   when(\"c_acctbal\" <= 3000, \"Class A\")\n",
    "#   .when(\"c_acctbal\" <= 6000, \"Class B\")\n",
    "#   .otherwise(\"Class C\")\n",
    "#   .alias(\"CustomerClassification\"))\n",
    "# df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "df_result = df_customer.select(df_customer.c_acctbal, expr(\"CASE WHEN c_acctbal <= 3000 THEN 'Class A' \" + \n",
    "                               \"WHEN c_acctbal <= 6000 THEN 'Class B' \" + \n",
    "                               \"ELSE 'Class C' END as CustomerClassification\"))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0da361f6-5107-45ee-a805-5ef041074eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes:\n",
    "\n",
    "* With the exception of string literal method, all other methods support CASE statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fea341c-b94d-433b-b362-18402143cbb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 10: - GROUP BY / AGGREGATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc98d913-2046-4cd3-a1a3-c26174b9cd49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col,expr, sum,avg,max,min,count\n",
    "\n",
    "#Method 1: Dot notation\n",
    "df_result = (df_customer.groupBy(df_customer.c_mktsegment)\n",
    "    .agg(\n",
    "        sum(df_customer.c_acctbal).alias(\"total_balance\"),\n",
    "        avg(df_customer.c_acctbal).alias(\"avg_balance\"),\n",
    "        max(df_customer.c_acctbal).alias(\"max_balance\"),\n",
    "        min(df_customer.c_acctbal).alias(\"min_balance\"),\n",
    "        count(\"*\").alias(\"groupCount\")\n",
    "    )\n",
    ").where(col(\"total_balance\") > 675000000) #Alternatively, expr() would also work, if you choose to.\n",
    "df_result.display()\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "df_result = (df_customer.groupBy(df_customer[\"c_mktsegment\"])\n",
    "    .agg(\n",
    "        sum(df_customer[\"c_acctbal\"]).alias(\"total_balance\"),\n",
    "        avg(df_customer[\"c_acctbal\"]).alias(\"avg_balance\"),\n",
    "        max(df_customer[\"c_acctbal\"]).alias(\"max_balance\"),\n",
    "        min(df_customer[\"c_acctbal\"]).alias(\"min_balance\"),\n",
    "        count(\"*\").alias(\"groupCount\")\n",
    "    )\n",
    ").where(col(\"total_balance\") > 675000000)\n",
    "df_result.display()\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "df_result = (df_customer.groupBy(col(\"c_mktsegment\"))\n",
    "    .agg(\n",
    "        sum(col(\"c_acctbal\")).alias(\"total_balance\"),\n",
    "        avg(col(\"c_acctbal\")).alias(\"avg_balance\"),\n",
    "        max(col(\"c_acctbal\")).alias(\"max_balance\"),\n",
    "        min(col(\"c_acctbal\")).alias(\"min_balance\"),\n",
    "        count(\"*\").alias(\"groupCount\")\n",
    "    )\n",
    ").where(col(\"total_balance\") > 675000000)\n",
    "df_result.display()\n",
    "\n",
    "#Method 4: Specifying column name as string literals\n",
    "df_result = (df_customer.groupBy((\"c_mktsegment\"))\n",
    "    .agg(\n",
    "        sum((\"c_acctbal\")).alias(\"total_balance\"),\n",
    "        avg((\"c_acctbal\")).alias(\"avg_balance\"),\n",
    "        max((\"c_acctbal\")).alias(\"max_balance\"),\n",
    "        min((\"c_acctbal\")).alias(\"min_balance\"),\n",
    "        count(\"*\").alias(\"groupCount\")\n",
    "    )\n",
    ").where(col(\"total_balance\") > 675000000)\n",
    "df_result.display()\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "df_result = (df_customer.groupBy(expr(\"c_mktsegment\"))\n",
    "    .agg(\n",
    "        sum(expr(\"c_acctbal\")).alias(\"total_balance\"),\n",
    "        avg(expr(\"c_acctbal\")).alias(\"avg_balance\"),\n",
    "        max(expr(\"c_acctbal\")).alias(\"max_balance\"),\n",
    "        min(expr(\"c_acctbal\")).alias(\"min_balance\"),\n",
    "        count(\"*\").alias(\"groupCount\")\n",
    "    )\n",
    ").where(col(\"total_balance\") > 675000000)\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30b71717-b8e0-4b8d-a573-a1929ae8f0ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes:\n",
    "\n",
    "In aggregate functions such as `sum()`, `avg()`, `max()` etc., you can use all the five methods to access columns.\n",
    "\n",
    "Want to reference aggregated columns?\n",
    "\n",
    "While referencing the aggregated columns in subsequent transformations, for example in a WHERE clause, you can use only `col()` and `expr()` methods and not the other column access methods. Because, the aggregated columns are assigned alias column names.\n",
    "\n",
    "In other words, you cannot reference columns of a DataFrame (like `df_result.total_balance`) inside the `.where()` clause before the DataFrame is created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60a4d43a-7d3f-4a39-907d-30c0a5c5ed3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 11: - Dataframe joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85c6060c-e1cd-4fd3-853d-08870cb274c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####11.1 - Single column join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5266b072-d5e1-4354-8ed2-8f4feb1c1d86",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.1.1 Method 1: Dot notation"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_n = df_nation.alias(\"df_n\")\n",
    "df_c = df_customer.alias(\"df_c\")\n",
    "\n",
    "df_twoDFs_joined = (\n",
    "        df_n\n",
    "        .join(df_c,\n",
    "            #Method 1: Dot notation\n",
    "            df_n.n_nationkey == df_c.c_nationkey,\n",
    "\n",
    "            \"inner\"\n",
    "        )\n",
    "        .select(df_n.n_nationkey.alias(\"NationKey\"),df_n.n_name.alias(\"NationName\"),df_c.c_name.alias(\"CustomerName\"))\n",
    "        .sort(col(\"CustomerName\").asc()) #With col(), you get to use aliased column name\n",
    "        #.sort(df_c.c_name.asc()) #With dot notation/square bracket notation, you'll need to use original column name rather than aliased name\n",
    "        #.sort(df_c[\"c_name\"].asc()) #With dot notation/square bracket notation, you'll need to use original column name rather than aliased name\n",
    "    )\n",
    "\n",
    "df_twoDFs_joined.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d26a45f-eaa4-4aa8-b3bc-60a2e4cf6d20",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.1.2 Method 2: Square bracket notation"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_n = df_nation.alias(\"df_n\")\n",
    "df_c = df_customer.alias(\"df_c\")\n",
    "\n",
    "df_twoDFs_joined = (\n",
    "        df_n\n",
    "        .join(df_c,\n",
    "\n",
    "            #Method 2: Square bracket notation\n",
    "            df_n[\"n_nationkey\"] == df_c[\"c_nationkey\"],\n",
    "\n",
    "            \"inner\"\n",
    "        )\n",
    "        .select(df_n[\"n_nationkey\"].alias(\"NationKey\"),df_n[\"n_name\"].alias(\"NationName\"),df_c[\"c_name\"].alias(\"CustomerName\"))\n",
    "        .sort(col(\"CustomerName\").asc()) #With col(), you get to use aliased column name\n",
    "        #.sort(df_c.c_name.asc()) #With dot notation/square bracket notation, you'll need to use original column name rather than aliased name\n",
    "        #.sort(df_c[\"c_name\"].asc()) #With dot notation/square bracket notation, you'll need to use original column name rather than aliased name\n",
    "    )\n",
    "\n",
    "df_twoDFs_joined.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f09323-b9f7-452f-878c-343e1f200740",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.1.3 Method 3: Using col()/column() function"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_n = df_nation.alias(\"df_n\")\n",
    "df_c = df_customer.alias(\"df_c\")\n",
    "\n",
    "df_twoDFs_joined = (\n",
    "        df_n\n",
    "        .join(df_c,\n",
    "\n",
    "            #Method 3: Using col()/column() function\n",
    "            col(\"df_n.n_nationkey\") == col(\"df_c.c_nationkey\"),            \n",
    "\n",
    "            \"inner\"\n",
    "        )\n",
    "        # column aliasing: all the three variations of column aliasing are supported, because dataframes are aliased before the current join command.\n",
    "        .select(col(\"df_n.n_nationkey\").alias(\"NationKey\"),col(\"df_n.n_name\").alias(\"NationName\"),col(\"df_c.c_name\").alias(\"CustomerName\"))\n",
    "        .sort(col(\"CustomerName\").asc()) #With col(), you get to use aliased column name\n",
    "        #.sort(df_c.c_name.asc()) #With dot notation/square bracket notation, you'll need to use original column name rather than aliased name\n",
    "        #.sort(df_c[\"c_name\"].asc()) #With dot notation/square bracket notation, you'll need to use original column name rather than aliased name\n",
    "    )\n",
    "\n",
    "df_twoDFs_joined.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5d103d-e772-4174-9780-9727873f760e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.1.4 Method 4: Specifying column name as string literals"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col,expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "# #IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "# df_n = df_nation.alias(\"df_n\")\n",
    "# df_c = df_customer.alias(\"df_c\")\n",
    "\n",
    "# df_twoDFs_joined = (\n",
    "#         df_n\n",
    "#         .join(df_c,\n",
    "       \n",
    "#             #Method 4: (DOES NOT WORK) Specifying column name as string literals\n",
    "#             #because, the pre-requisite with this option is the joining column names must be exactly the same in both the dataframes\n",
    "#             #\"n_nationkey\",  \n",
    "\n",
    "#             \"inner\"\n",
    "#         )\n",
    "\n",
    "#         .select(df_n.n_nationkey.alias(\"NationKey\"),df_n.n_name.alias(\"NationName\"),df_c.c_name.alias(\"CustomerName\"))\n",
    "#         .sort(col(\"CustomerName\").asc()) #With col(), you get to use aliased column name\n",
    "#         #.sort(df_c.c_name.asc()) #With dot notation/square bracket notation, you'll need to use original column name rather than aliased name\n",
    "#         #.sort(df_c[\"c_name\"].asc()) #With dot notation/square bracket notation, you'll need to use original column name rather than aliased name\n",
    "#     )\n",
    "\n",
    "# df_twoDFs_joined.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6327d37c-3f28-426e-8b00-3adc6ae83024",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.1.5 Method 5: Using expr() / selectExpr() function"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_n = df_nation.alias(\"df_n\")\n",
    "df_c = df_customer.alias(\"df_c\")\n",
    "\n",
    "df_twoDFs_joined = (\n",
    "        df_n\n",
    "        .join(df_c,\n",
    "\n",
    "            #Method 5: Using expr() / selectExpr() function\n",
    "            #Both variations of the below syntax works\n",
    "            #Note: Even though expr() works here too, but do you really want to use it hear as the function name of expr() isn't self-descriptive in this context?\n",
    "            #expr(\"df_n.n_nationkey\") == expr(\"df_c.c_nationkey\"),           \n",
    "            expr(\"df_n.n_nationkey == df_c.c_nationkey\"),    \n",
    "\n",
    "            \"inner\"\n",
    "        )\n",
    "        # column aliasing: all the three variations of column aliasing are supported, because dataframes are aliased before the current join command.\n",
    "        .select(expr(\"df_n.n_nationkey\").alias(\"NationKey\"),expr(\"df_n.n_name\").alias(\"NationName\"),expr(\"df_c.c_name\").alias(\"CustomerName\"))\n",
    "        .sort(expr(\"CustomerName\").asc()) \n",
    "        #.sort(df_c.c_name.asc()) #With dot notation/square bracket notation, you'll need to use original column name rather than aliased name\n",
    "        #.sort(df_c[\"c_name\"].asc()) #With dot notation/square bracket notation, you'll need to use original column name rather than aliased name\n",
    "    )\n",
    "\n",
    "df_twoDFs_joined.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a479b6f-0b8f-4e67-b037-1c8be6350fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Assumption**: The observations as below are based on the pre-requisite that the dataframes are aliased and assigned to variables before they are referenced in the dataframe joins, as demonstrated in the above examples.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* With the exception of the column name as string literal method, all other four methods can be used to access the dataframe columns for its join\n",
    "* To be able to use column name as string literal method, the condition to be satisfied is that the joining column names must be the same in both the dataframes.\n",
    "* To referencing the aliased column names, we need to use col() method or expr() method.\n",
    "* Even though expr() works even in dataframe joins, do you really want to use it in joins, given the function name doesn't reflect the purpose of its usage in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7230929c-70db-43ba-a79f-50a4e3dbf2a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####11.2 - Multi-column join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec2680a0-4ec5-4b5e-905e-79d0f6919a26",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sample data part 2 - for multi-column join"
    }
   },
   "outputs": [],
   "source": [
    "#29,999,795\n",
    "df_lineitem = spark.read.table(\"samples.tpch.lineitem\")\n",
    "display(df_lineitem)\n",
    "\n",
    "#4000,000\n",
    "df_partsupp = spark.read.table(\"samples.tpch.partsupp\")\n",
    "display(df_partsupp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be391562-145c-4e99-9e36-d32933b3b0f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.2.1 Method 1: Dot notation"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_li = df_lineitem.alias(\"df_li\")  \n",
    "df_ps = df_partsupp.alias(\"df_ps\")\n",
    "\n",
    "df_multiplejoin2DFs = (\n",
    "        df_li\n",
    "        .join(df_ps,\n",
    "        #Method 1: Dot notation along with aliased dataframe names \n",
    "        [(df_li.l_partkey == df_ps.ps_partkey) &\n",
    "        (df_li.l_suppkey == df_ps.ps_suppkey)],\n",
    "        \n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(df_li.l_partkey.alias(\"PartKey\"), df_li.l_suppkey.alias(\"SuppKey\"), df_ps.ps_availqty.alias(\"AvailableQty\"))\n",
    "    .sort(col(\"PartKey\").asc()) #using aliased column name \n",
    ")\n",
    "\n",
    "df_multiplejoin2DFs.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2daf15fa-01f7-47f6-b44d-58f6f2c41251",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.2.2 Method 2: Square bracket notation"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_li = df_lineitem.alias(\"df_li\")  \n",
    "df_ps = df_partsupp.alias(\"df_ps\")\n",
    "\n",
    "df_multiplejoin2DFs = (\n",
    "        df_li\n",
    "        .join(df_ps,\n",
    "\n",
    "        #Method 2: Square bracket notation\n",
    "        [(df_li[\"l_partkey\"] == df_ps[\"ps_partkey\"]) &\n",
    "        (df_li[\"l_suppkey\"] == df_ps[\"ps_suppkey\"])],  \n",
    "        \n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(df_li[\"l_partkey\"].alias(\"PartKey\"), df_li[\"l_suppkey\"].alias(\"SuppKey\"), df_ps[\"ps_availqty\"].alias(\"AvailableQty\"))\n",
    "    .sort(col(\"PartKey\").asc()) #using aliased column name \n",
    ")\n",
    "\n",
    "df_multiplejoin2DFs.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95d6e7c-e55d-4c9d-8811-20e4980ae08d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.2.3 Method 3: Using col()/column() function"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_li = df_lineitem.alias(\"df_li\")  \n",
    "df_ps = df_partsupp.alias(\"df_ps\")\n",
    "\n",
    "df_multiplejoin2DFs = (\n",
    "        df_li\n",
    "        .join(df_ps,\n",
    "\n",
    "        #Method 3: Using col()/column() function + aliased dataframes\n",
    "        [(col(\"df_li.l_partkey\") == col(\"df_ps.ps_partkey\")) &\n",
    "        (col(\"df_li.l_suppkey\") == col(\"df_ps.ps_suppkey\"))],\n",
    "        \n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(col(\"df_li.l_partkey\").alias(\"PartKey\"), col(\"df_li.l_suppkey\").alias(\"SuppKey\"), col(\"df_ps.ps_availqty\").alias(\"AvailableQty\"))\n",
    "    .sort(col(\"PartKey\").asc()) #using aliased column name \n",
    ")\n",
    "\n",
    "df_multiplejoin2DFs.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eff341b-f51f-4a6c-9d54-b14efcfe58a5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.2.4 Method 4: Specifying column name as string literals"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, expr\n",
    "\n",
    "# #Dataframe aliasing\n",
    "# #IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "# df_li = df_lineitem.alias(\"df_li\")  \n",
    "# df_ps = df_partsupp.alias(\"df_ps\")\n",
    "\n",
    "# df_multiplejoin2DFs = (\n",
    "#         df_li\n",
    "#         .join(df_ps,\n",
    "\n",
    "#         #Method 4: Specifying column name as string literals: (DOES NOT WORK) using columns names as string literal doesn't work because, the joining column names are not exactly the same in both the dataframes\n",
    "#         #[\"l_partkey\",\"l_suppkey\"],\n",
    "        \n",
    "#         \"inner\"\n",
    "#     )\n",
    "#     .select(df_li.l_partkey.alias(\"PartKey\"), df_li.l_suppkey.alias(\"SuppKey\"), df_ps.ps_availqty.alias(\"AvailableQty\"))\n",
    "#     .sort(col(\"PartKey\").asc()) #using aliased column name \n",
    "# )\n",
    "\n",
    "# df_multiplejoin2DFs.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a699af4-137a-4467-8071-e5b45eeae217",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.2.5 Method 5: Using expr() / selectExpr() function"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_li = df_lineitem.alias(\"df_li\")  \n",
    "df_ps = df_partsupp.alias(\"df_ps\")\n",
    "\n",
    "df_multiplejoin2DFs = (\n",
    "        df_li\n",
    "        .join(df_ps,\n",
    "\n",
    "        #Method 5: Using expr() / selectExpr() function + aliased dataframes\n",
    "        #Both variations of the below syntax works\n",
    "        # [(expr(\"df_li.l_partkey\") == expr(\"df_ps.ps_partkey\")) &\n",
    "        # (expr(\"df_li.l_suppkey\") == expr(\"df_ps.ps_suppkey\"))],\n",
    "\n",
    "        [(expr(\"df_li.l_partkey == df_ps.ps_partkey\")) &\n",
    "        (expr(\"df_li.l_suppkey == df_ps.ps_suppkey\"))],\n",
    "        \n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(expr(\"df_li.l_partkey\").alias(\"PartKey\"), expr(\"df_li.l_suppkey\").alias(\"SuppKey\"), expr(\"df_ps.ps_availqty\").alias(\"AvailableQty\"))\n",
    "    .sort(expr(\"PartKey\").asc()) #using aliased column name \n",
    ")\n",
    "\n",
    "df_multiplejoin2DFs.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4253ad3-b009-448d-b452-0f9dcb9ccc0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes: Same as those with the single column joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b58b0c69-2424-4e35-9d08-045c42df3ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####11.3 Self-join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3382878-70e8-46a6-9e9b-0863ba7ab084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Preview of sample data for self-join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "881498b6-8297-4176-b8be-a7cc6217a50d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| emp_id | emp_name | job_title | manager_id |\n",
    "|--------|----------|-----------|------------|\n",
    "|101|David|CEO|None|\n",
    "|102|Kevin|GM|101|\n",
    "|103|Ben|EM|102|\n",
    "|104|Mike|PM|103|\n",
    "|105|Jack|Data Engineer|104|\n",
    "|106|Melissa|Data Engineer|104|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0cc4beb-aba9-4e5e-912b-815e860d2ec5",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756474237460}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Sample data part 3 - for self-join"
    }
   },
   "outputs": [],
   "source": [
    "employee_data = (\n",
    "    (101,\"David\",\"CEO\",None),\n",
    "    (102,\"Kevin\",\"GM\",101),\n",
    "    (103,\"Ben\",\"EM\",102),\n",
    "    (104,\"Mike\",\"PM\",103),\n",
    "    (105,\"Jack\",\"Data Engineer\",104),\n",
    "    (106,\"Melissa\",\"Data Engineer\",104)\n",
    "  )\n",
    "employee_schema = (\"emp_id int, emp_name string, job_title string, manager_id int\")\n",
    "\n",
    "df_employee = spark.createDataFrame(data = employee_data, schema = employee_schema)\n",
    "\n",
    "df_employee.select(\"*\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b654194-cc02-4387-b85a-7cc7fb24fc9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.3.1 Method 1: Dot notation"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_emp = df_employee.alias(\"df_emp\")\n",
    "df_mgr = df_employee.alias(\"df_mgr\")\n",
    "\n",
    "#Method 1: Dot notation\n",
    "df_result_selfjoin = (\n",
    "    df_emp\n",
    "    .join(\n",
    "        df_mgr,\n",
    "        df_emp.manager_id == df_mgr.emp_id,\n",
    "        \"left_outer\"\n",
    "    )\n",
    "    .select(\n",
    "        df_emp.emp_id.alias(\"Employee_Id\"),\n",
    "        df_emp.emp_name.alias(\"Employee_Name\"),\n",
    "        df_emp.job_title.alias(\"Employee_Job_Title\"),\n",
    "        df_mgr.emp_id.alias(\"Manager_Id\"),\n",
    "        df_mgr.emp_name.alias(\"Manager_Name\"),\n",
    "        df_mgr.job_title.alias(\"Manager_Job_Title\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result_selfjoin.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d0f86b-9b4d-45a2-970a-ef22b1ab9d14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.3.2 Method 2: Square bracket notation"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_emp = df_employee.alias(\"df_emp\")\n",
    "df_mgr = df_employee.alias(\"df_mgr\")\n",
    "\n",
    "#Method 2: Square bracket notation\n",
    "df_result_selfjoin = (\n",
    "    df_emp\n",
    "    .join(\n",
    "        df_mgr,\n",
    "        df_emp[\"manager_id\"] == df_mgr[\"emp_id\"],\n",
    "        \"left_outer\"\n",
    "    )\n",
    "    .select(\n",
    "        df_emp[\"emp_id\"].alias(\"Employee_Id\"),\n",
    "        df_emp[\"emp_name\"].alias(\"Employee_Name\"),\n",
    "        df_emp[\"job_title\"].alias(\"Employee_Job_Title\"),\n",
    "        df_mgr[\"emp_id\"].alias(\"Manager_Id\"),\n",
    "        df_mgr[\"emp_name\"].alias(\"Manager_Name\"),\n",
    "        df_mgr[\"job_title\"].alias(\"Manager_Job_Title\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result_selfjoin.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba043b7-edd1-41ee-b0c8-5732f32f0adf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.3.3 Method 3: Using col()/column() function"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_emp = df_employee.alias(\"df_emp\")\n",
    "df_mgr = df_employee.alias(\"df_mgr\")\n",
    "\n",
    "#Method 3: Using col()/column() function\n",
    "df_result_selfjoin = (\n",
    "    df_emp\n",
    "    .join(\n",
    "        df_mgr,\n",
    "        col(\"df_emp.manager_id\") == col(\"df_mgr.emp_id\"),\n",
    "        \"left_outer\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"df_emp.emp_id\").alias(\"Employee_Id\"),\n",
    "        col(\"df_emp.emp_name\").alias(\"Employee_Name\"),\n",
    "        col(\"df_emp.job_title\").alias(\"Employee_Job_Title\"),\n",
    "        col(\"df_mgr.emp_id\").alias(\"Manager_Id\"),\n",
    "        col(\"df_mgr.emp_name\").alias(\"Manager_Name\"),\n",
    "        col(\"df_mgr.job_title\").alias(\"Manager_Job_Title\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result_selfjoin.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f384b11-625c-4a16-8c48-539ece9e4fe6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.3.4 Method 4: Specifying column name as string literals"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, expr\n",
    "\n",
    "# #Dataframe aliasing\n",
    "# #IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "# df_emp = df_employee.alias(\"df_emp\")\n",
    "# df_mgr = df_employee.alias(\"df_mgr\")\n",
    "\n",
    "# #Method 4: Specifying column name as string literals\n",
    "# #ATTENTION: Not tried because for the string literal to work in a join, column names must be identical on either side of the dataframe join. In case of self-joins, the column names being joined wouldn't be the same.\n",
    "\n",
    "# df_result_selfjoin.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4651b56-8ae0-4603-9dfb-c67d8f33dcee",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756475977482}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "11.3.5 Method 5: Using expr() / selectExpr() function"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "#Dataframe aliasing\n",
    "#IMPORTANT: make sure variable name and string name passed to alias method are exactly the same.\n",
    "df_emp = df_employee.alias(\"df_emp\")\n",
    "df_mgr = df_employee.alias(\"df_mgr\")\n",
    "\n",
    "#Method 5: Using expr() / selectExpr() function\n",
    "df_result_selfjoin = (\n",
    "    df_emp\n",
    "    .join(\n",
    "        df_mgr,\n",
    "        expr(\"df_emp.manager_id == df_mgr.emp_id\"),\n",
    "        \"left_outer\"\n",
    "    )\n",
    "    .select(\n",
    "        expr(\"df_emp.emp_id\").alias(\"Employee_Id\"),\n",
    "        expr(\"df_emp.emp_name\").alias(\"Employee_Name\"),\n",
    "        expr(\"df_emp.job_title\").alias(\"Employee_Job_Title\"),\n",
    "        expr(\"df_mgr.emp_id\").alias(\"Manager_Id\"),\n",
    "        expr(\"df_mgr.emp_name\").alias(\"Manager_Name\"),\n",
    "        expr(\"df_mgr.job_title\").alias(\"Manager_Job_Title\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result_selfjoin.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e344c68-d8fc-4f85-a857-38b6045cfaea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notes: Same comments as with the single-column joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "469502a2-18cd-4682-bd8e-3dcd0802650e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Key Takeways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef74038e-627a-4358-b763-48013ba80990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Strengths and limitations - Method-wise\n",
    "\n",
    "| Method | Strengths | Limitations / When Not to Use |\n",
    "|--------|-----------|-------------------------------|\n",
    "|`col(\"colName\")` |\t- Works in almost all scenarios <br> - Can reference *aliased* and *aggregated* columns (e.g. in `WHERE / ORDER BY`) | - Cannot be used as new name when *adding* columns  use **string literals**<br> - Cannot be used to *rename* columns  use **string literals** |\n",
    "|Dot Notation <br>`(df.colName)` |\t- Concise, intuitive, very readable <br> - Familiar to OOP/Python users <br> - Gives code a **consistent look** |\t- Fails if special characters in column names  use **col()** <br> - Cannot access aliased columns  use **col()** <br> - Cannot be used as new name when *adding* columns  use **string literals**<br> - Cannot be used to *rename* columns  use **string literals** |\n",
    "|String Literals <br>`(\"colName\")` | - Required for: <br>  new column name while adding new column with `withColumn()` <br>  Renaming with `withColumnRenamed()`\t| - Very limited outside of those cases |\n",
    "|`expr(\"sql_expression\")`| - Can do everything **col()** can <br> - Allows **SQL-like expressions** directly as strings <br> - Great for analysts moving from SQL \t| - Best kept for SQL-style expressions (avoid using it as a **col()** replacement for clarity) |\n",
    "|Square Brackets <br>`(df[\"colName\"])` |- Same as dot notation. Additionally, supports special characters in column names |\t- Cannot access *aliased* columns  use **col()** <br> - Cannot be used as new name when *adding* columns  use **string literals**<br> - Cannot be used to *rename* columns  use **string literals** <br> - Slightly more verbose, less elegant |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8cec35f-5eb5-475a-8cde-cf397fe638f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Best practice recommendations\n",
    "\n",
    "A **hybrid approach** is recommended, as no single method covers all scenarios for accessing columns in a DataFrame.\n",
    "\n",
    "**Option A**\n",
    "\n",
    "* Use **col()** for most transformations, including when working with aliased/aggregated columns or column names containing special characters.\n",
    "* Use **string literals** only when:\n",
    "  - Specifying a new column name with `withColumn()`, or\n",
    "  - Renaming a column with `withColumnRenamed()`.\n",
    "\n",
    "**Option B**\n",
    "\n",
    "* Use **dot notation** for concise, readable code. It is familiar to Python users and provides a consistent coding style.\n",
    "* Use **col()** when working with aliased/aggregated columns or columns with special characters.\n",
    "* Use **string literals** only when:\n",
    "  - Specifying a new column name with `withColumn()`, or\n",
    "  - Renaming a column with `withColumnRenamed()`.\n",
    "\n",
    "**Either option works well**; choosing between them is a matter of what suits you best.\n",
    "\n",
    "**Notes**\n",
    "1. Reserve **expr()** for SQL-style expressions. Avoid using **expr()** as a replacement for **col()** to maintain clarity.\n",
    "2. While **square bracket notation** can perform everything that dot notation does, it is not recommended as a best practice due to its slightly more verbose and cumbersome nature. If you still prefer square bracket notation over dot notation, you will still need a hybrid approach along with col() for aliased/aggregated columns and string literals when creating or renaming columns."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8720878723921497,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DataFrame - PySpark Column Access methods - comparison",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
