{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b86ccf-74bc-4c3d-954b-216879a11773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark Joins - Common Pitfalls and Practical Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "149a8838-d9f7-463e-8508-8c4cb1529bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##ER Diagram: tpch database\n",
    "\n",
    "Note:  tpch is a database in samples catalog that's **available in Databricks free edition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b60b5abe-8f12-4729-beb7-375a87902837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"./ERDiagram_Databricks_Samples_catalog_tpch_db.png\" alt=\"ERDiagram_Databricks_Samples_catalog_tpch_db.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6cf469a-108d-4725-bc3f-8c5465b986f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Example datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c50ccb65-bf47-42ee-b32f-d009d53a9d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Example data 1 (samples.tpch database tables)\n",
    "\n",
    "Note: The **samples catalog** and the **tpch schema** are available by default in Databricks free edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a02999c-815b-4e53-9add-ff7037b42dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customer = spark.read.table(\"samples.tpch.customer\")\n",
    "df_orders = spark.read.table(\"samples.tpch.orders\")\n",
    "df_supplier = spark.read.table(\"samples.tpch.supplier\")\n",
    "\n",
    "df_nation = spark.read.table(\"samples.tpch.nation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f31e246-1eb1-4d42-b541-7267d884ec10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Example data 2 (demo on deduping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fecb0262-4634-457c-8ee4-7835aeadee78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#(i). Invoice dataframe v1: Each set of duplicate rows have the same values across all columns\n",
    "df_invoice_exact_duplicates = spark.createDataFrame(\n",
    "    [\n",
    "        (3301, 29620, 100.00, datetime.strptime(\"2025-10-01\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-01 11:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "        (3301, 29620, 100.00, datetime.strptime(\"2025-10-01\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-01 11:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "        (3301, 29620, 100.00, datetime.strptime(\"2025-10-01\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-01 11:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "\n",
    "        (3302, 29620, 50.00, datetime.strptime(\"2025-10-02\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-02 12:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "        (3302, 29620, 50.00, datetime.strptime(\"2025-10-02\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-02 12:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "\n",
    "        (3303, 29621, 200.00, datetime.strptime(\"2025-10-03\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-03 15:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "        (3303, 29621, 200.00, datetime.strptime(\"2025-10-03\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-03 15:00:00\", \"%Y-%m-%d %H:%M:%S\"))\n",
    "    ],\n",
    "    \"InvoiceKey: int, suppkey: bigint, InvoiceAmount: double, InvoiceDate: date, LastUpdated: timestamp\"\n",
    ")\n",
    "\n",
    "#(ii). Invoice dataframe v2: Duplicate on business keys only (i.e. [suppkey + InvoiceKey] in this example)\n",
    "df_invoice_duplicate_business_key = spark.createDataFrame(\n",
    "    [\n",
    "        (3301, 29620, 100.00, datetime.strptime(\"2025-10-01\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-01 11:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "        (3301, 29620, 120.00, datetime.strptime(\"2025-10-01\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-01 11:15:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "        (3301, 29620, 150.00, datetime.strptime(\"2025-10-01\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-01 11:30:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "\n",
    "        (3302, 29620, 50.00, datetime.strptime(\"2025-10-02\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-02 12:30:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "        (3302, 29620, 55.00, datetime.strptime(\"2025-10-02\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-02 12:45:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "\n",
    "        (3303, 29621, 200.00, datetime.strptime(\"2025-10-03\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-03 15:10:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "        (3303, 29621, 275.00, datetime.strptime(\"2025-10-03\", \"%Y-%m-%d\"), datetime.strptime(\"2025-10-03 15:20:00\", \"%Y-%m-%d %H:%M:%S\"))\n",
    "    ],\n",
    "    \"InvoiceKey: int, suppkey: bigint, InvoiceAmount: double, InvoiceDate: date, LastUpdated: timestamp\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c62ee86-ff90-4757-b7ca-d5101fb67174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Example data 3 (demo on data matching case-sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31fb209-0c92-4a93-9c23-5600804431ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Employee dataframe\n",
    "df_employee = spark.createDataFrame(\n",
    "    [\n",
    "        ('E1001','Kevin','DPT501'),\n",
    "        ('E1002','David','DPT502'),\n",
    "        ('E1003','Ben','Dpt502'),\n",
    "        ('E1004','Linda','DPT503')\n",
    "    ],\n",
    "    \"EmpID: string, Name: string, DeptID: string\"\n",
    ")\n",
    "\n",
    "df_employee.display()\n",
    "\n",
    "#2. Department dataframe\n",
    "df_department = spark.createDataFrame(\n",
    "    [\n",
    "        ('DPT501','Engineering'),\n",
    "        ('DPT502','Sales'),\n",
    "        ('DPT503','Marketing'),\n",
    "        ('DPT525','IT') \n",
    "    ],\n",
    "    \"DeptID: string, Name: string\"\n",
    ")\n",
    "\n",
    "df_department.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11f63b2d-26b3-497f-b62c-9c62316f7171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Example data 4 (demo on handling NULLs in joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf6aba7-50f9-40a7-9ec3-5a17682e9fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#(i) order dataframe with 1 null (to demonstrate how to handle nulls in the joins)\n",
    "df_orders_nulls_demo_1Null = spark.createDataFrame(\n",
    "    (\n",
    "        (50001, 'C101'),\n",
    "        (None, 'C101'),\n",
    "        (50002, 'C102'),\n",
    "        #(None, 'C103'), #Enabled for scenario 3 only\n",
    "        (50004, 'C104'),\n",
    "        (50005, 'C105')\n",
    "    ),\n",
    "    (\"orderid: int, customerid: string\")\n",
    ")\n",
    "\n",
    "#(ii) order dataframe with 2 nulls (to demonstrate how to handle nulls in the joins)\n",
    "df_orders_nulls_demo_2Nulls = spark.createDataFrame(\n",
    "    (\n",
    "        (50001, 'C101'),\n",
    "        (None, 'C101'),\n",
    "        (50002, 'C102'),\n",
    "        (None, 'C103'),\n",
    "        (50004, 'C104'),\n",
    "        (50005, 'C105')\n",
    "    ),\n",
    "    (\"orderid: int, customerid: string\")\n",
    ")\n",
    "\n",
    "#(iii) order_lineitems dataframe\n",
    "df_lineitems_nulls_demo = spark.createDataFrame(\n",
    "    (\n",
    "        (50001, 1, 'Apple',5.0, 2),\n",
    "        (50001, 2, 'Banana',5.0, 2),\n",
    "        (None, 3, 'Orange',30.0, 1),\n",
    "        (50002, 1, 'Carrots',3.0, 1),\n",
    "        (50002, 2, 'Potato',4.0, 2),\n",
    "        (None, 1, 'Lemon',25.0, 2),\n",
    "        (None, 2, 'Milk',50.0, 2),\n",
    "        (50004, 1, 'Bread',5.0, 2),\n",
    "        (50005, 1, 'Rice',5.0, 2),\n",
    "        (50005, 2, 'Shampoo',5.0, 2)\n",
    "    ),\n",
    "    (\"orderid: int, lineitemnumber: int, productkey: string, price: double, quantity: int\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd520f54-a7e4-437a-89d1-bf8f7e96ad79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Example data 5 (demo on many-to-many relationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10db83f2-ca7f-4547-a8d5-b6a44e4218f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#(i) student df\n",
    "df_student = spark.createDataFrame(\n",
    "    (\n",
    "        (101, 'David'),\n",
    "        (102, 'Vanessa'),\n",
    "        (103, 'John')\n",
    "    ),\n",
    "    (\"id: int, name: string\")\n",
    ")\n",
    "\n",
    "#(ii) subject df\n",
    "df_subject = spark.createDataFrame(\n",
    "    (\n",
    "        (501, 'Mathematics'),\n",
    "        (502, 'Biology'),\n",
    "        (503, 'History'),\n",
    "        (504, 'English')\n",
    "    ),\n",
    "    (\"id: int, name: string\")\n",
    ")\n",
    "\n",
    "#(iii) student-subject df (bridge table)\n",
    "df_bridge_student_subject = spark.createDataFrame(\n",
    "    (\n",
    "        (101,501),\n",
    "        (101,502),\n",
    "        (101,504),\n",
    "        (102,502),\n",
    "        (102,503),\n",
    "        (103,501),\n",
    "        (103,502),\n",
    "        (103,503),\n",
    "        (103,504)\n",
    "    ),\n",
    "    (\"student_id: int, subject_id: int\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9331abff-1fb6-411b-ae7e-f3cb61fed7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Example data 6 (demo on One Problem, Many Solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec4bf508-7842-4836-8d43-5cecf148595a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "\n",
    "# (i) serviceprovider df\n",
    "df_serviceprovider = spark.createDataFrame(\n",
    "    [\n",
    "        (201, 'Service Provider 201'),\n",
    "        (202, 'Service Provider 202'),\n",
    "        (203, 'Service Provider 203'),\n",
    "        (204, 'Service Provider 204')\n",
    "    ],\n",
    "    \"id: int, name: string\"\n",
    ")\n",
    "\n",
    "# (ii) invoice df\n",
    "df_invoice = spark.createDataFrame(\n",
    "    [\n",
    "        ('INV201001', 201, Decimal('10000.00'), datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\"), 'Paid'),\n",
    "        ('INV201002', 201, Decimal('15000.00'), datetime.strptime(\"2022-01-02\", \"%Y-%m-%d\"), 'Pending'),\n",
    "        ('INV202001', 202, Decimal('2500.00'), datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\"), 'Paid'),\n",
    "        ('INV203001', 203, Decimal('9000.00'), datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\"), 'Pending')\n",
    "    ],\n",
    "    \"invoice_code: string, serviceprovider_id: int, invoice_amount: decimal(18,2), invoice_date: date, invoice_payment_status: string\"\n",
    ")\n",
    "\n",
    "# (iii) invoice category df\n",
    "df_invoicecategory = spark.createDataFrame(\n",
    "    [\n",
    "        ('Invoice Category: 0-5K',Decimal('00.00'),Decimal('5000.00')),\n",
    "        ('Invoice Category: 5K-10K',Decimal('5000.01'),Decimal('10000.00')),\n",
    "        ('Invoice Category: 10K-20K',Decimal('10000.01'),Decimal('20000.00')),\n",
    "        ('Invoice Category: Over 20K',Decimal('20000.01'),Decimal('999999.00'))\n",
    "    ],\n",
    "    \"category_description: string, lower_bound: decimal(18,2), upper_bound: decimal(18,2)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e229d5f-72e1-46e0-91e2-ee67fcf9c852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Example data 7 (demo on role-playing dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af037a4-87a8-4a6e-adcf-bbc58adeb324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# (i) DimDate df\n",
    "df_dimdate = spark.createDataFrame(\n",
    "    [\n",
    "        (20251001,datetime.strptime('2025-10-01', \"%Y-%m-%d\"),4,'Wednesday',10,'October', 4, 2025, False, 'Q4-2025'),\n",
    "        (20251002,datetime.strptime('2025-10-02', \"%Y-%m-%d\"),5,'Thursday',10,'October', 4, 2025, False, 'Q4-2025'),\n",
    "        (20251003,datetime.strptime('2025-10-03', \"%Y-%m-%d\"),6,'Friday',10,'October', 4, 2025, False, 'Q4-2025'),\n",
    "        (20251004,datetime.strptime('2025-10-04', \"%Y-%m-%d\"),7,'Saturday',10,'October', 4, 2025, True, 'Q4-2025'),\n",
    "        (20251005,datetime.strptime('2025-10-05', \"%Y-%m-%d\"),1,'Sunday',10,'October', 4, 2025, True, 'Q4-2025'),\n",
    "        (20251006,datetime.strptime('2025-10-06', \"%Y-%m-%d\"),2,'Monday',10,'October', 4, 2025, False, 'Q4-2025'),\n",
    "        (20251007,datetime.strptime('2025-10-07', \"%Y-%m-%d\"),3,'Tuesday',10,'October', 4, 2025, False, 'Q4-2025'),\n",
    "        (20251008,datetime.strptime('2025-10-08', \"%Y-%m-%d\"),4,'Wednesday',10,'October', 4, 2025, False, 'Q4-2025'),\n",
    "        (20251009,datetime.strptime('2025-10-09', \"%Y-%m-%d\"),5,'Thursday',10,'October', 4, 2025, False, 'Q4-2025'),\n",
    "        (20251010,datetime.strptime('2025-10-10', \"%Y-%m-%d\"),6,'Friday',10,'October', 4, 2025, False, 'Q4-2025')\n",
    "    ],\n",
    "    (\"DateKey: int, FullDate: Date, DayOfWeek: int, DayName: string, Month: int, MonthName: string, Quarter: int, Year: int, IsHoliday: boolean, FiscalPeriod: string\")\n",
    ")\n",
    "\n",
    "df_dimdate.display()\n",
    "\n",
    "#(ii) FactSales df\n",
    "df_factsales = spark.createDataFrame(\n",
    "    [\n",
    "        (1000001, 20251001, 20251002, 20251003, 'PKey001', 'CKey301', 10, 100.00),\n",
    "        (1000002, 20251004, 20251005, 20251006, 'PKey002', 'CKey302', 5, 10.00),\n",
    "        (1000003, 20251006, 20251007, 20251008, 'PKey003', 'CKey303', 3, 150.00),\n",
    "        (1000004, 20251007, 20251008, 20251010, 'PKey004', 'CKey304', 12, 16.00)\n",
    "    ],\n",
    "    (\"transaction_id: int, OrderDateKey: int, ShipDateKey: int, DeliveryDateKey: int, ProductKey: string, CustomerKey: string, QuantitySold: int, UnitPrice: double\")\n",
    ")\n",
    "\n",
    "df_factsales.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "115601a8-41c0-4604-8ff8-912e4a3f08f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1.0 Common Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c4ef5cf-5cc1-4b73-9822-556e647cbc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.1 Tricky LEFT OUTER JOIN\n",
    "\n",
    "**Scenario**: Say, we want to output every customer record from `samples.tpch.customer` table, along with details of only those order records in `samples.tpch.orders` that have an orderdate >= '1998-08-01'\n",
    "\n",
    "The tricky bit: The filter in this scenario is on the right dataframe rather than on the left dataframe.\n",
    "\n",
    "**Solution Discussion**:\n",
    "\n",
    "One may think of applying the filter on orders data either:\n",
    "- (Option 1): while joining the two dataframes\n",
    "- (Option 2): after joining the two dtaframes\n",
    "\n",
    "Given that this is a left outer join. And, more importantly, the filter is on the right dataframe:\n",
    "- Option 1 would return CORRECT output.\n",
    "- Option 2 would return INCORRECT output.\n",
    "\n",
    "The reason being:\n",
    "* The option 1 would first filter out orders to get only those orders that have an orderdate >= '1998-08-01'. Then, joins them with customer dataframe.\n",
    "* Whereas, option 2 first joins customers dataframe with orders dataframe. Then, filters the combined dataframe on orderdate >= '1998-08-01'\n",
    "\n",
    "The following two cells will demonstrate each of these scenarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f86cb4c8-d1a9-4e63-be90-5714492ea381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Correct result (filtered as part of join condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa9fceb-3e68-4338-b2a9-0af6d5e9d458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 1 (samples.tpch database tables)\n",
    "df_cust_alias = df_customer.alias(\"df_cust_alias\") \n",
    "df_ord_alias = df_orders.alias(\"df_ord_alias\")\n",
    "\n",
    "df_result = (\n",
    "    df_cust_alias\n",
    "    .join(df_ord_alias, \n",
    "        #join condition:\n",
    "        [\n",
    "        (df_cust_alias.c_custkey == df_ord_alias.o_custkey) &\n",
    "        (df_ord_alias.o_orderdate >= \"1998-08-01\")\n",
    "        ]\n",
    "        ,\n",
    "\n",
    "        \"leftouter\"\n",
    "   )\n",
    "    .select(\n",
    "    df_cust_alias.c_custkey.alias(\"cust_key\"),\n",
    "    df_cust_alias.c_name.alias(\"cust_name\"),\n",
    "    df_ord_alias.o_orderkey.alias(\"order_key\"),\n",
    "    df_ord_alias.o_orderdate.alias(\"order_date\"),\n",
    "    df_ord_alias.o_orderstatus.alias(\"order_status\"),\n",
    "    df_ord_alias.o_totalprice.alias(\"order_total_price\")\n",
    "    )\n",
    "    #.filter(col(\"order_date\") >= \"1998-08-01\") #In case of LEFT OUTER Join, filtering data over here would result in incorrect results!!!\n",
    ")\n",
    "\n",
    "df_result.count() #As a best practice, remove the count() call before promoting code to higher environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5ba634-5714-40f9-a118-95034f696b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note: The customer table in samples.tpch database has only 750,000 records. Whereas, the above output has 750,022 rows, which is correct. Because, one customer may have 0 to many order records. That way, 22 customers have 2 orders each in orders table that satisfied the filter `o_orderdate >= \"1998-08-01\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32d8f4c2-efe2-4cfb-bc68-655418822312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Incorrect result (filtered after the join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc1b57df-a0bd-4218-a12d-4eeb0bfc0007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 1 (samples.tpch database tables)\n",
    "df_cust_alias = df_customer.alias(\"df_cust_alias\") \n",
    "df_ord_alias = df_orders.alias(\"df_ord_alias\")\n",
    "\n",
    "df_result = (\n",
    "    df_cust_alias\n",
    "    .join(df_ord_alias, \n",
    "        #join condition:\n",
    "        [\n",
    "        (df_cust_alias.c_custkey == df_ord_alias.o_custkey) #&\n",
    "        #(df_ord_alias.o_orderdate >= \"1998-08-01\")\n",
    "        ]\n",
    "        ,\n",
    "\n",
    "        \"leftouter\"\n",
    "   )\n",
    "    .select(\n",
    "    df_cust_alias.c_custkey.alias(\"cust_key\"),\n",
    "    df_cust_alias.c_name.alias(\"cust_name\"),\n",
    "    df_ord_alias.o_orderkey.alias(\"order_key\"),\n",
    "    df_ord_alias.o_orderdate.alias(\"order_date\"),\n",
    "    df_ord_alias.o_orderstatus.alias(\"order_status\"),\n",
    "    df_ord_alias.o_totalprice.alias(\"order_total_price\")\n",
    "    )\n",
    "    .filter(col(\"order_date\") >= \"1998-08-01\") #In case of LEFT OUTER Join, filtering data over here would result in incorrect results!!!\n",
    ")\n",
    "\n",
    "df_result.count() #As a best practice, remove the count() call before promoting code to higher environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cf68153-49df-45a2-b93b-8b1fd0955b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Key Takeaways: Tricky LEFT OUTER JOIN\n",
    "\n",
    "If your scenario is a :\n",
    "* Left outer join, AND\n",
    "* the filter is on the RIGHT dataframe rather than on the left one\n",
    "\n",
    "Then, the filter should be part of the join condition itself rather than applying the filter after the join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eedcf9b-27f5-4c0e-b124-8b935f67e51a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Hypothetical scenario: What if it were an INNER JOIN instead?\n",
    "\n",
    "**Scenario**: What if the join scenario in the preceding example was such that we just need an INNER JOIN rather than a LEFT-OUTER-JOIN. In such a case, would it make any difference regardless of whether we applied the filter while joining dataframes (i.e. option 1) or after joining the dataframes (i.e. option 2)?\n",
    "\n",
    "In an INNER JOIN scenario, either option would return correct results.\n",
    "\n",
    "However, from performance perspective, which one would perform better? That needs to be explored separately. Mind you, the performance also depends on size of data being processed among many other factors. In case of small datasets, either option may provide satisfactory performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce07e116-e765-475c-a143-07003435ddd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.2 Deduping data before a join\n",
    "\n",
    "**Scenario**: Say, you want to join two dataframes. However, one of the dataframes has duplicates records. In such a scenario, how to eliminate duplicates from making it to the output?\n",
    "\n",
    "**Duplicate data - Background info.:**\n",
    "\n",
    "Types of duplicate data:\n",
    "* Exact duplicates: Two or more rows have identical values across all columns\n",
    "* Duplicates on business key: Two or more rows have matching values on business key(s)\n",
    "\n",
    "The deduping solution depends on the type of duplicate data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23266ec9-52dc-4f0a-b519-d224c7444c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Scenario 1: Exact duplicates - pick any one of the duplicates\n",
    "\n",
    "**Duplicates Scenario**: Exact duplicates, wherein two or more rows have identical values across all columns.\n",
    "\n",
    "For example, in the *df_invoice_exact_duplicates* as below, the InvoiceKey = 3301 has three duplicate rows with the same values across all columns.\n",
    "\n",
    "| InvoiceKey | suppkey | InvoiceAmount | InvoiceDate | LastUpdated |\n",
    "|------------|---------|---------------|-------------|-------------|\n",
    "| 3301 | 29620 | 100 | 2025-10-01 | 2025-10-01T11:00:00.000+00:00 |\n",
    "| 3301 | 29620 | 100 | 2025-10-01 | 2025-10-01T11:00:00.000+00:00 |\n",
    "| 3301 | 29620 | 100 | 2025-10-01 | 2025-10-01T11:00:00.000+00:00 |\n",
    "| 3302 | 29620 | 50 | 2025-10-02 | 2025-10-02T12:00:00.000+00:00 |\n",
    "| 3302 | 29620 | 50 | 2025-10-02 | 2025-10-02T12:00:00.000+00:00 |\n",
    "| 3303 | 29621 | 200 | 2025-10-03 | 2025-10-03T15:00:00.000+00:00 |\n",
    "| 3303 | 29621 | 200 | 2025-10-03 | 2025-10-03T15:00:00.000+00:00 |\n",
    "\n",
    "**Solution**: Dedupe the *df_invoice_exact_duplicates* dataframe using `dropDuplicates()` or `distinct()` before joining the two dataframes.\n",
    "\n",
    "Tip: `drop_duplicates()` is an alias for `dropDuplicates()`\n",
    "\n",
    "**Limitation**: Entire row would have to be duplicate across every column, to be able to be deduplicated by this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c040aa5d-f64c-47e9-b293-7a6be20de736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset 1: Example data 1 (samples.tpch database tables)\n",
    "#Ref dataset 2: Example data 2 (demo on deduping)\n",
    "df_supp_alias = df_supplier.alias(\"df_supp_alias\") \n",
    "\n",
    "#Dedupe by entire row: use either dropDuplicates() or distinct()\n",
    "#df_invoice_alias = df_invoice_exact_duplicates.alias(\"df_invoice_alias\").distinct()\n",
    "df_invoice_alias = df_invoice_exact_duplicates.alias(\"df_invoice_alias\").dropDuplicates()\n",
    "\n",
    "df_result = (\n",
    "    df_supp_alias\n",
    "    .join(df_invoice_alias, \n",
    "\n",
    "        (df_supp_alias.s_suppkey == df_invoice_alias.suppkey),\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "    df_supp_alias.s_suppkey.alias(\"supplier_key\"),\n",
    "    df_supp_alias.s_name.alias(\"supplier_name\"),\n",
    "    df_invoice_alias.InvoiceKey.alias(\"Invoice_key\"),\n",
    "    df_invoice_alias.InvoiceAmount.alias(\"Invoice_amount\"),\n",
    "    df_invoice_alias.InvoiceDate.alias(\"Invoice_date\"),\n",
    "    df_invoice_alias.LastUpdated.alias(\"Last_updated\")\n",
    "    )\n",
    ").sort(col(\"supplier_key\"), col(\"Invoice_key\"))\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05fc260a-8a47-471e-99a3-67cd8176604e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Scenario 2A: Duplicates on business key - pick any one of the duplicates\n",
    "\n",
    "**Dedupe Scenario**: Duplicates were found on  business keys (suppkey + InvoiceKey). Pick any one of the duplicate rows regardless of the values in rest of the columns were duplicates or not.\n",
    "\n",
    "For example, in the *df_invoice_duplicate_business_key* as below, the InvoiceKey = 3301 has three duplicate business keys. Additionally, Invoice Amount and Last Updated column values differed between the duplicate rows.\n",
    "\n",
    "| InvoiceKey | suppkey | InvoiceAmount | InvoiceDate | LastUpdated |\n",
    "|------------|---------|---------------|-------------|-------------|\n",
    "| 3301 | 29620 | 100 | 2025-10-01 | 2025-10-01T11:00:00.000+00:00 |\n",
    "| 3301 | 29620 | 120 | 2025-10-01 | 2025-10-01T11:15:00.000+00:00 |\n",
    "| 3301 | 29620 | 150 | 2025-10-01 | 2025-10-01T11:30:00.000+00:00 |\n",
    "| 3302 | 29620 | 50 | 2025-10-02 | 2025-10-02T12:30:00.000+00:00 |\n",
    "| 3302 | 29620 | 55 | 2025-10-02 | 2025-10-02T12:45:00.000+00:00 |\n",
    "| 3303 | 29621 | 200 | 2025-10-03 | 2025-10-03T15:10:00.000+00:00 |\n",
    "| 3303 | 29621 | 275 | 2025-10-03 | 2025-10-03T15:20:00.000+00:00 |\n",
    "\n",
    "**Solution**: Use `dropDuplicates([SubsetColList_optional])` by passing it appropriate business keys that you want to dedupe on.\n",
    "\n",
    "*Tip*: `SubsetColList_optional` param ALWAYS requires a `list[]` to be passed as value, even if just one column name is to be passed as param.\n",
    "\n",
    "**Limitation**: We cannot guarantee which of those multiple duplicate rows would be picked up as winner. On a small dataset such as in this demo, it was picking up the first row in each set of duplicates as winner. But, it isn't something that we can guarantee. In other words, deduplicated output is non-deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa7af30e-f100-42cc-89d5-afc5b37c867d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Last_updated\":251},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761537839066}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset 1: Example data 1 (samples.tpch database tables)\n",
    "#Ref dataset 2: Example data 2 (demo on deduping)\n",
    "df_supp_alias = df_supplier.alias(\"df_supp_alias\") \n",
    "\n",
    "#Dedupe using business keys only (i.e. [suppkey + InvoiceKey] in this example)\n",
    "df_invoice_alias = df_invoice_duplicate_business_key.alias(\"df_invoice_alias\").dropDuplicates([\"suppkey\",\"InvoiceKey\"])\n",
    "\n",
    "df_result = (\n",
    "    df_supp_alias\n",
    "    .join(df_invoice_alias, \n",
    "\n",
    "        (df_supp_alias.s_suppkey == df_invoice_alias.suppkey),\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "    df_supp_alias.s_suppkey.alias(\"supplier_key\"),\n",
    "    df_supp_alias.s_name.alias(\"supplier_name\"),\n",
    "    df_invoice_alias.InvoiceKey.alias(\"Invoice_key\"),\n",
    "    df_invoice_alias.InvoiceAmount.alias(\"Invoice_amount\"),\n",
    "    df_invoice_alias.InvoiceDate.alias(\"Invoice_date\"),\n",
    "    df_invoice_alias.LastUpdated.alias(\"Last_updated\")\n",
    "    )\n",
    ").sort(col(\"supplier_key\"), col(\"Invoice_key\"))\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "132bb5a9-d8c3-48bb-bfdb-53d5d17075fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Scenario 2B: Duplicates on business key - dedupe using tie-breaker columns\n",
    "\n",
    "**Dedupe Scenario**: Duplicates on business keys (suppkey + InvoiceKey) were found. Dedupe them on the business key + tie-breaker column(s).\n",
    "\n",
    "For example, in the *df_invoice_duplicate_business_key* as below, the InvoiceKey = 3301 has three duplicate business keys. Sort them on the tie-breaker column (LastUpdated) in desc() order and pick the latest of such duplicate rows. \n",
    "\n",
    "| InvoiceKey | suppkey | InvoiceAmount | InvoiceDate | LastUpdated |\n",
    "|------------|---------|---------------|-------------|-------------|\n",
    "| 3301 | 29620 | 100 | 2025-10-01 | 2025-10-01T11:00:00.000+00:00 |\n",
    "| 3301 | 29620 | 120 | 2025-10-01 | 2025-10-01T11:15:00.000+00:00 |\n",
    "| 3301 | 29620 | 150 | 2025-10-01 | 2025-10-01T11:30:00.000+00:00 |\n",
    "| 3302 | 29620 | 50 | 2025-10-02 | 2025-10-02T12:30:00.000+00:00 |\n",
    "| 3302 | 29620 | 55 | 2025-10-02 | 2025-10-02T12:45:00.000+00:00 |\n",
    "| 3303 | 29621 | 200 | 2025-10-03 | 2025-10-03T15:10:00.000+00:00 |\n",
    "| 3303 | 29621 | 275 | 2025-10-03 | 2025-10-03T15:20:00.000+00:00 |\n",
    "\n",
    "**Solution approach**: \n",
    "* Step 1: Partition data on business key(s)\n",
    "* Step 2: Sort data asc/desc on tie-breaker columns\n",
    "* Step 3: Assign row numbers in each partition\n",
    "* Step 4: Pick first row from within each partition\n",
    "\n",
    "**Pros**: The deduped output would be deterministic, based on the tie-breaker column that we specify in deduping.\n",
    "\n",
    "Note: If there were duplicates even on LastUpdated column for a given business key, then the result could be a non-deterministic. In such a case, additional columns may need to be considered to make the output deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3380f6c3-ad29-48ff-ae1b-6246be2cbc45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "#Ref dataset 1: Example data 1 (samples.tpch database tables)\n",
    "#Ref dataset 2: Example data 2 (demo on deduping)\n",
    "df_supp_alias = df_supplier.alias(\"df_supp_alias\") \n",
    "\n",
    "#Dedupe using business keys + row_number() on Last_updated column desc.\n",
    "w = (Window.partitionBy(\n",
    "    df_invoice_duplicate_business_key.suppkey, df_invoice_duplicate_business_key.InvoiceKey)\n",
    "    .orderBy(df_invoice_duplicate_business_key.LastUpdated.desc())\n",
    ")\n",
    "\n",
    "df_invoice_alias = (df_invoice_duplicate_business_key.alias(\"df_invoice_alias\")\n",
    "    .withColumn(\"duplicateRowNo\", row_number().over(w))\n",
    "    .filter(col(\"duplicateRowNo\") == 1)\n",
    ")\n",
    "\n",
    "df_result = (\n",
    "    df_supp_alias\n",
    "    .join(df_invoice_alias, \n",
    "\n",
    "        (df_supp_alias.s_suppkey == df_invoice_alias.suppkey),\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "    df_supp_alias.s_suppkey.alias(\"supplier_key\"),\n",
    "    df_supp_alias.s_name.alias(\"supplier_name\"),\n",
    "    df_invoice_alias.InvoiceKey.alias(\"Invoice_key\"),\n",
    "    df_invoice_alias.InvoiceAmount.alias(\"Invoice_amount\"),\n",
    "    df_invoice_alias.InvoiceDate.alias(\"Invoice_date\"),\n",
    "    df_invoice_alias.LastUpdated.alias(\"Last_updated\")\n",
    "    )\n",
    ").sort(col(\"supplier_key\"), col(\"Invoice_key\"))\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c07f130-7a53-407a-995e-4f7bbdeeb54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Key Takeaways: Deduping scenarios\n",
    "\n",
    "| Deduplication Scenario | Recommended deduping approach |\n",
    "|------------------------|-----------------------|\n",
    "| Scenario 1: Exact duplicates | Pick any one of the duplicate rows by using `dropDuplicate()` |\n",
    "| Scenario 2A: Duplicates on business key columns. As deduped output, any one of the duplicate rows is acceptable. | Dedupe by using `dropDuplicate(ListOfBizKeys)` by explicitly specifying business key columns to dedupe on. Limitation: No guarantee on which one of the multiple duplicate rows is picked up as deduped row. The result might be **non-deterministic** |\n",
    "| Scenario 2B: Duplicates on business key columns. (similar to scenario 22A). But, additionally, we want to be able to specify tie-breaker criteria. | Four steps in the solution: Step 1: Partition dataframe on business key columns; Step 2: Sort data on the tie-breaker columns in asc/desc order as required; Step 3: Assign row numbers within each set of duplicate rows; Step 4: Pick the first row in each group of duplicates. Benefit: Finer control on deduping. The output will be **deterministic**. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c3bf263-3de5-4ef5-8fe8-a71a5817bbd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.3 Data matching is case-sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ff0cb4d-1692-4b81-b363-1c7bd97ea68d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Scenario 1: joins on columns of string data type\n",
    "\n",
    "Say, we want to join two tables - `employee` and `department` DeptID. In department table, the DeptID values have a prefix of DPT (all in upper case). e.g. DPT501. However, in employee table: One of the employee records has a DeptID of 'Dpt502' rather than all upper case DPT502.\n",
    "\n",
    "In such a case, in the output, would empID: E1003, be able to find a matching department record on 'Dpt502' == 'DPT502'?\n",
    "\n",
    "**df_employee**\n",
    "| EmpID | Name | DeptID |\n",
    "|-------|------|--------|\n",
    "| E1001 | Kevin | DPT501 |\n",
    "| E1002 | David | DPT502 |\n",
    "| E1003 | Ben | Dpt502 (Notice the prefix was Dpt rather than DPT) |\n",
    "| E1004 | Linda | DPT503 |\n",
    "\n",
    "\n",
    "**df_department**\n",
    "| DeptID | Name |\n",
    "|--------|------|\n",
    "| DPT501 | Engineering |\n",
    "| DPT502 | Sales |\n",
    "| DPT503 | Marketing |\n",
    "| DPT525 | IT |\n",
    "\n",
    "**Answer**: No. empID: E1003 wouldn't find a matching department record. Because, the matching on column values is case-sensitive, as demonstrated in the below query. In employee table, for empID: E1003, the DeptID value was 'Dpt502'. Whereas, in Department table the DeptID was 'DPT502' (all upper case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffea29e5-6e30-467a-a695-2e89b266ab90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**df_employee**\n",
    "| EmpID | Name | DeptID |\n",
    "|-------|------|--------|\n",
    "| E1001 | Kevin | DPT501 |\n",
    "| E1002 | David | DPT502 |\n",
    "| E1003 | Ben | Dpt502 |\n",
    "| E1004 | Linda | DPT503 |\n",
    "\n",
    "\n",
    "**df_department**\n",
    "| DeptID | Name |\n",
    "|--------|------|\n",
    "| DPT501 | Engineering |\n",
    "| DPT502 | Sales |\n",
    "| DPT503 | Marketing |\n",
    "| DPT525 | IT |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5f1200-9745-4506-9dbb-3727ce875531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_emp_alias = df_employee.alias(\"df_emp_alias\") \n",
    "df_dept_alias = df_department.alias(\"df_dept_alias\")\n",
    "\n",
    "df_result = (\n",
    "    df_emp_alias\n",
    "    .join(df_dept_alias, \n",
    "\n",
    "        (df_emp_alias.DeptID == df_dept_alias.DeptID),\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "    df_emp_alias.EmpID.alias(\"Employee_ID\"),\n",
    "    df_emp_alias.Name.alias(\"Employee_Name\"),\n",
    "    df_emp_alias.DeptID.alias(\"DeptID_AsInEmpTable\"),\n",
    "    df_dept_alias.DeptID.alias(\"DeptID_AsInDeptTable\"),\n",
    "    df_dept_alias.Name.alias(\"Department_Name\")\n",
    "    )\n",
    "    .sort(col(\"Employee_ID\").asc())\n",
    ")\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab904b42-050c-401f-a475-9ee6b4d2800b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Scenario 2: Filtering on hard-coded string literals\n",
    "\n",
    "For example, in the below code, at line# 11/12:\n",
    "- if the filter value were specified as 'Japan', it would return no rows\n",
    "- if the filter value were specified as 'JAPAN', it would return data\n",
    "\n",
    "Because, in the underlying table, the values for this column was all in upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9a5fc9-7bdd-41c2-809c-1581f8d3c146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset 1: Example data 1 (samples.tpch database tables)\n",
    "df_nat_alias = df_nation.alias(\"df_nat_alias\")\n",
    "df_cust_alias = df_customer.alias(\"df_cust_alias\") \n",
    "\n",
    "df_result = (\n",
    "    df_nat_alias\n",
    "    .join(df_cust_alias, \n",
    "\n",
    "        (df_nat_alias.n_nationkey == df_cust_alias.c_nationkey) & \n",
    "        #(df_nat_alias.n_name == 'Japan'), #Filter is case-sensitive\n",
    "        (df_nat_alias.n_name == 'JAPAN'), #Filter is case-sensitive\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "    df_nat_alias.n_nationkey.alias(\"nation_key\"),\n",
    "    df_nat_alias.n_name.alias(\"nation_name\"),\n",
    "    df_cust_alias.c_custkey.alias(\"cust_key\"),\n",
    "    df_cust_alias.c_name.alias(\"cust_name\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de38fe2e-9b51-4c90-88e1-f5b3470996c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Key takeaway: Data matching is case-sensitive\n",
    "\n",
    "The data matching in Spark (pySpark / SQL) is case-sensitive. It effects:\n",
    "* table joins,\n",
    "* filters.\n",
    "\n",
    "So, please be mindful of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ae4292-1b19-44cf-8243-2331557d017f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from samples.tpch.nation\n",
    "where n_name = 'Japan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58a2e583-54d4-489d-8a65-4f4d5aa45c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.4 Handling NULLs in Joins? Proceed with Caution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a468bc-fddf-498f-9104-2c0fc2ef5e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Background Knowledge\n",
    "\n",
    "The default behaviour of `NULL` / `None` values in data:\n",
    "* When a `NULL` is compared with any not-null value, it returns `NULL`\n",
    "* When a `NULL` is compared with another NULL value, it still returns `NULL`\n",
    "\n",
    "Whereas, when `eqNullSafe()` is used in comparison of column values:\n",
    "* When a `NULL` is compared with any not-null value, `false` is returned instead of the `NULL` as in default behaviour\n",
    "* When a `NULL` is compared with another NULL value, `true` is returned instead of the `NULL` as in default behaviour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8bc244e-9c27-4e2d-be1d-1acfe9eba9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Scenario 1: What if I don't handle NULLs in joins?\n",
    "\n",
    "**Join Scenario**: Say, we have two dataframes to join:\n",
    "* df_orders_nulls_demo_1Null\n",
    "* df_lineitems_nulls_demo\n",
    "\n",
    "As shown in the sample data as below, we want to join both these dataframes on `orderid` column. However, as can be noticed in the data, there is a single NULL value in the `df_orders`. And, three NULL values in the `df_lineitems`.\n",
    "\n",
    "As a first scenario, let's see what happens if don't do anything special on these NULLs in the key column being used in joining these two dataframes.\n",
    "\n",
    "**1. df_orders_nulls_demo_1Null**:\n",
    "\n",
    "| customerid |orderid | \n",
    "|------------|--------|\n",
    "| C101 | 50001 |\n",
    "| C101 | null |\n",
    "| C102 | 50002 |\n",
    "| C104 | 50004 |\n",
    "| C105 | 50005 |\n",
    "\n",
    "**2. df_lineitems_nulls_demo**:\n",
    "\n",
    "| orderid | lineitemnumber | productkey | price | quantity |\n",
    "|---------|----------------|------------|-------|----------|\n",
    "| 50001 | 1 | Apple | 5 | 2 |\n",
    "| 50001 | 2 | Banana | 5 | 2 |\n",
    "| null | 3 | Orange | 30 | 1 |\n",
    "| 50002 | 1 | Carrots | 3 | 1 |\n",
    "| 50002 | 2 | Potato | 4 | 2 |\n",
    "| null | 1 | Lemon | 25 | 2 |\n",
    "| null | 2 | Milk | 50 | 2 |\n",
    "| 50004 | 1 | Bread | 5 | 2 |\n",
    "| 50005 | 1 | Rice | 5 | 2 |\n",
    "| 50005 | 2 | Shampoo | 5 | 2 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c91cfa2-7ff1-472c-8450-1e7144f14a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 3 (demo on handling NULLs in joins)\n",
    "df_ord_dummy_alias = df_orders_nulls_demo_1Null.alias(\"df_ord_dummy_alias\") \n",
    "df_lineitem_dummy_alias = df_lineitems_nulls_demo.alias(\"df_lineitem_dummy_alias\")\n",
    "\n",
    "df_result = (\n",
    "    df_ord_dummy_alias\n",
    "    .join(df_lineitem_dummy_alias, \n",
    "\n",
    "        (df_ord_dummy_alias.orderid == df_lineitem_dummy_alias.orderid),\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "    df_ord_dummy_alias.orderid.alias(\"Order_Id_In_Orders\"),\n",
    "    df_ord_dummy_alias.customerid.alias(\"Customer_Id\"),\n",
    "    df_lineitem_dummy_alias.orderid.alias(\"Order_Id_In_LineItem\"),\n",
    "    df_lineitem_dummy_alias.lineitemnumber.alias(\"LineItemNumber\"),\n",
    "    df_lineitem_dummy_alias.productkey.alias(\"Product_Key\"),\n",
    "    df_lineitem_dummy_alias.price.alias(\"Price\"),    \n",
    "    df_lineitem_dummy_alias.quantity.alias(\"Quantity\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b313a215-1b3e-4172-b1e8-9c046b2d5fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Comments - scenario 1 (default behaviour on NULLs in a join)\n",
    "* Ignoring rows that have NULLs on the join key, the remaining 7 rows were returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "423691a3-7ff4-4ef9-9c62-0b3f0e5fa328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Scenario 2: Single NULL in join Key in one of the two tables\n",
    "\n",
    "**Join Scenario**: This scenario is very similar to that of scenario 1, with the exception of the variation in the join condition as below:\n",
    "\n",
    "In scenario 1 - join condition:\n",
    "* `(df_ord_dummy_alias.orderid == df_lineitem_dummy_alias.orderid)`\n",
    "\n",
    "In scenario 2 - join condition:\n",
    "* `(df_ord_dummy_alias.orderid.eqNullSafe(df_lineitem_dummy_alias.orderid))`\n",
    "\n",
    "As a result of using `eqNullSafe` in scenario 2, the query treats as if NULL == NULL.\n",
    "\n",
    "Accordingly, the output returns 10 rows.\n",
    "\n",
    "Please note that in this scenario, in df_orders there was only a single NULL value in the key column (orderId), while in the 2nd df (i.e. df_lineitem) there were 3 NULLs in the key column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6a7710-bc98-49f8-826a-cff2d495a4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 3 (demo on handling NULLs in joins)\n",
    "df_ord_dummy_alias = df_orders_nulls_demo_1Null.alias(\"df_ord_dummy_alias\") \n",
    "df_lineitem_dummy_alias = df_lineitems_nulls_demo.alias(\"df_lineitem_dummy_alias\")\n",
    "\n",
    "df_result = (\n",
    "    df_ord_dummy_alias\n",
    "    .join(df_lineitem_dummy_alias, \n",
    "\n",
    "        #(df_ord_dummy_alias.orderid == df_lineitem_dummy_alias.orderid), # scenario 1\n",
    "        (df_ord_dummy_alias.orderid.eqNullSafe(df_lineitem_dummy_alias.orderid)), # scenario 2\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "    df_ord_dummy_alias.orderid.alias(\"Order_Id_In_Orders\"),\n",
    "    df_ord_dummy_alias.customerid.alias(\"Customer_Id\"),\n",
    "    df_lineitem_dummy_alias.orderid.alias(\"Order_Id_In_LineItem\"),\n",
    "    df_lineitem_dummy_alias.lineitemnumber.alias(\"LineItemNumber\"),\n",
    "    df_lineitem_dummy_alias.productkey.alias(\"Product_Key\"),\n",
    "    df_lineitem_dummy_alias.price.alias(\"Price\"),    \n",
    "    df_lineitem_dummy_alias.quantity.alias(\"Quantity\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c29e632-2f12-41ba-a9af-69ab578f00f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Scenario 3: Multiple NULLs in join key in both the tables\n",
    "\n",
    "**Join scenario 3**: Continuing from scenario 2, in this scenario, say we have two NULL values in the key column of df_orders, unlike just a single NULL value as in scenario 2.\n",
    "\n",
    "Let's see what happens when both of the two dataframes have more than one NULL value in the join column(s).\n",
    "\n",
    "Note: For this particular scenario, a modified version of the df_order dataframe as below is used:\n",
    "* `df_orders_nulls_demo_2Nulls`\n",
    "\n",
    "Notice that when compared to the previous df_orders, this one has an extra rows (where orderId was NULL and customerid = C103)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ac283f1-5225-4be7-bb0f-b6d1d5f38916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**df_orders_nulls_demo_2Nulls**:\n",
    "\n",
    "| customerid | orderid |\n",
    "|------------|---------|\n",
    "| C101 | 50001 |\n",
    "| C101 | null |\n",
    "| C102 | 50002 |\n",
    "| C103 (new to scenario 3) | null |\n",
    "| C104 | 50004 |\n",
    "| C105 | 50005 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b65c772-3a14-4e3d-9821-bb6a3fb15117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 3 (demo on handling NULLs in joins)\n",
    "df_ord_dummy_alias = df_orders_nulls_demo_2Nulls.alias(\"df_ord_dummy_alias\") \n",
    "df_lineitem_dummy_alias = df_lineitems_nulls_demo.alias(\"df_lineitem_dummy_alias\")\n",
    "\n",
    "df_result = (\n",
    "    df_ord_dummy_alias\n",
    "    .join(df_lineitem_dummy_alias, \n",
    "\n",
    "        #(df_ord_dummy_alias.orderid == df_lineitem_dummy_alias.orderid),\n",
    "        (df_ord_dummy_alias.orderid.eqNullSafe(df_lineitem_dummy_alias.orderid)),\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "    df_ord_dummy_alias.orderid.alias(\"Order_Id_In_Orders\"),\n",
    "    df_ord_dummy_alias.customerid.alias(\"Customer_Id\"),\n",
    "    df_lineitem_dummy_alias.orderid.alias(\"Order_Id_In_LineItem\"),\n",
    "    df_lineitem_dummy_alias.lineitemnumber.alias(\"LineItemNumber\"),\n",
    "    df_lineitem_dummy_alias.productkey.alias(\"Product_Key\"),\n",
    "    df_lineitem_dummy_alias.price.alias(\"Price\"),    \n",
    "    df_lineitem_dummy_alias.quantity.alias(\"Quantity\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result.sort(col(\"Customer_id\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ef2da38-861e-46e9-8e7d-98acbc9a7a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Handling NULLs in Joins? Proceed with Caution\n",
    "\n",
    "Given that in scenario 3 (Multiple NULLs in join key in both the tables), we have two NULLs in the join key column of 1st df and three NULLs in the join key column of 2nd df, it resulted in a cartisian product of 6 rows (2x3) representing the NULL as key, which isn't correct.\n",
    "\n",
    "So, if we want to use `eqNullSafe` while joining two dataframes to handle NULLs in key columns, we have to ensure:\n",
    "* one of the two dataframes being joined cannot have more than 1 NULL value in the key column.\n",
    "* if both dataframes have more than 1 NULL value in the key column, that would result in INCORRECT output.\n",
    "\n",
    "Moreover, grouping all the rows with the NULL as key column may not necessarily be correct from business perspective. NULL stands for unknown. That way, grouping multiple UNKNOWNs into one group may or may not necessarily make business sense.\n",
    "\n",
    "Additionally, the key columns on which we want to join dataframes, are expected to be always containing NOT NULL values. Having said that, if there were lots of NULLs in the key columns, it is most likely a data quality issue. And, it would require an investigation of its own.\n",
    "\n",
    "For these reasons, proceed with caution if you want to handle NULLs while joining two columns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdfe18d5-6404-4a55-b881-739de594332b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Key Takeaways: Handling NULLs in joins\n",
    "\n",
    "* Ideally, DO NOT ALLOW NULLs in key columns. \n",
    "* If NULLs are allowed in key-columns, do have data quality checks in place regardless of whether you are handling NULLs in key columns using `eqNullSafe` or using any other approach\n",
    "* If handling NULLs in key columns, ensure there is no more than 1 NULL value in the key column of one of the two dataframes being joined to avoid incorrect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0381df8-c804-48d1-8ed3-42a91528ae83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.5 Handling Many-to-Many relationship using a bridge\n",
    "\n",
    "**Theory**: \n",
    "\n",
    "A couple of basic examples on many-to-many relationship:\n",
    "* Student-Subjects relationship: One student may study many subjects and a subject can be taken up by many students.\n",
    "* Book-Author relationship: One author may write many books and a book may be written by many authors.\n",
    "\n",
    "A typical solution for many-to-many relationship is to have a bridge table between the dataframes that were otherwise in a many-to-many relationship.\n",
    "\n",
    "**Join scenario**: Say, we have a student table and subject table. Obviously, they were in a many-to-many relationship. So, to resolve the many-to-many relationship into one-to-many relationship, a bridge table was introduced in between the two.\n",
    "\n",
    "Once a many-to-many relationship is resolved into two sets of one-to-many relationships, writing joins between the three dataframes is no different to any multi-table join.\n",
    "\n",
    "**df_student dataframe**:\n",
    "\n",
    "| id | name |\n",
    "|----|------|\n",
    "| 101 | David |\n",
    "| 102 | Sarah |\n",
    "| 103 | John |\n",
    "\n",
    "**df_subject dataframe**:\n",
    "| id | name |\n",
    "|----|------|\n",
    "| 501 | Mathematics |\n",
    "| 502 | Biology |\n",
    "| 503 | History |\n",
    "| 504 | English |\n",
    "\n",
    "**df_bridge_student_subject dataframe**:\n",
    "| student_id | subject_id |\n",
    "|------------|------------|\n",
    "| 101 | 501 |\n",
    "| 101 | 502 |\n",
    "| 101 | 504 |\n",
    "| 102 | 502 |\n",
    "| 102 | 503 |\n",
    "| 103 | 501 |\n",
    "| 103 | 502 |\n",
    "| 103 | 503 |\n",
    "| 103 | 504 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b3dc4b-e0b0-4e65-861a-1e7cf30bddbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 4 (demo on many-to-many relationship)\n",
    "df_student_alias = df_student.alias(\"df_student_alias\") \n",
    "df_subject_alias = df_subject.alias(\"df_subject_alias\")\n",
    "df_bridge_stud_sub_alias = df_bridge_student_subject.alias(\"df_bridge_stud_sub_alias\")\n",
    "\n",
    "df_result = (\n",
    "    df_student_alias\n",
    "    #Join Student to the bridge table\n",
    "    .join(df_bridge_stud_sub_alias, \n",
    "\n",
    "        (df_student_alias.id == df_bridge_stud_sub_alias.student_id),\n",
    "\n",
    "        \"inner\"\n",
    "    )\n",
    "    #Join Subject to the bridge table\n",
    "    .join(df_subject_alias, \n",
    "\n",
    "        (df_bridge_stud_sub_alias.subject_id == df_subject_alias.id),\n",
    "    \n",
    "        \"inner\"      \n",
    "    )\n",
    "    .select(\n",
    "    df_student_alias.id.alias(\"student_id\"),\n",
    "    df_student_alias.name.alias(\"student_name\"),\n",
    "    df_subject_alias.id.alias(\"subject_id\"),\n",
    "    df_subject_alias.name.alias(\"subject_name\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"student_id\").asc(), col(\"subject_id\").asc()) #For code brevity, prefer referencing aliased column names in sort() by using col()\n",
    "\n",
    ")\n",
    "\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3274cb0-2209-4ff8-8bb3-6ad21362ec3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Key Takeaways: Many-to-Many relationship\n",
    "\n",
    "If two tables are in a many-to-many relationship, such data models would need to be revised to resolve the many-to-many relationship. One approach is to introduce a bridge table in between them, to convert it into two one-to-many relationship.\n",
    "\n",
    "Once a bridge table is introduced between two tables that were otherwise in many-to-many relationship, writing joins between them is no different to joining multi-tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ad73f19-e4a2-43d2-b4ab-1eb2918e5611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.6 Use Broadcast join with care!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c52130c2-390c-4c1a-8900-9378f696f355",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Theory\n",
    "\n",
    "Broadcast join is a performance optimization technique, wherein you can pass smaller of the two dataframes being joined, to the `broadcast()` function. The table is broadcast by the driver, who copies it to all worker nodes.\n",
    "\n",
    "When executing joins, Spark automatically broadcasts tables less than 10MB; however, we may adjust this threshold to broadcast even larger tables. If youre running a driver with a lot of memory (32GB+), you can safely raise the broadcast thresholds to something like 200MB.\n",
    "\n",
    "Nevertheless, Databricks recommends to always explicitly broadcast smaller tables using hints or PySpark broadcast function, even if AQE can automatically broadcast smaller tables for us.\n",
    "\n",
    "Ref: https://www.databricks.com/discover/pages/optimize-data-workloads-guide\n",
    "\n",
    "So, let's see an example of PySpark `broadcast()` function.\n",
    "\n",
    "**Scenario**: Join `df_customer` dataframe (contains 750,000 rows) with `df_nation` dataframe (which has only 25 rows). Given the `df_nation` dataframe is very small, let's use `broadcast()` function on the `df_nation` dataframe.\n",
    "\n",
    "Note: Both `df_customer` and `df_nation` dataframes were populated earlier in this notebook from the `samples.tpch database`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d6164a7-c33a-41ef-bdba-30b805b04262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Broadcast join example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19137cb5-54fa-4346-994c-45fdd268912c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast, col\n",
    "\n",
    "#Ref dataset: Example data 1 (samples.tpch database tables)\n",
    "#df_customer.count() #750,000\n",
    "#df_nation.count() #25 rows\n",
    "\n",
    "#Tip: Alternatively, you can call broadcast() function, before using the df in a join \n",
    "#df_nat_bc_alias = broadcast(df_nation)\n",
    "\n",
    "df_result = (\n",
    "    df_customer.alias(\"df_cust_alias\")\n",
    "    #Calling broadcast() function on a dataframe marks a DataFrame as small enough for use in broadcast joins.\n",
    "    .join(broadcast(df_nation).alias(\"df_nat_bc_alias\"),\n",
    "          \n",
    "          #Join condition\n",
    "          #Note: Due to inline-aliasing of dataframes, we cannot use dot notation to access columns.\n",
    "          [col(\"df_cust_alias.c_nationkey\") == col(\"df_nat_bc_alias.n_nationkey\")],\n",
    "          \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"df_cust_alias.c_custkey\").alias(\"customer_key\"),\n",
    "        col(\"df_cust_alias.c_name\").alias(\"customer_name\"), \n",
    "        col(\"df_nat_bc_alias.n_name\").alias(\"nation_name\")\n",
    "    )\n",
    ")\n",
    "\n",
    "#df_result.explain(extended=False) #To analyze Broadcast Join\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "231f77c1-1c56-4b2a-b4d0-19dd7ec02633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Key Takeaway: Use broadcast join with care!\n",
    "\n",
    "Consider broadcasting a dataframe only when it is small. 10MB is the default size to broadcast tables automatically. However, we may adjust this threshold to broadcast even larger tables. If youre running a driver with a lot of memory (32GB+), you can safely raise the broadcast thresholds to something like 200MB.\n",
    "\n",
    "If a dataframe being broadcast is not small as we intended it to be, it could result in Out of Memory (OOM) errors.\n",
    "\n",
    "As recommended by Databricks, never broadcast a table bigger than 1GB because broadcast happens via the driver and a 1GB+ table could cause OOM on the driver.\n",
    "\n",
    "So, use broadcast join with care!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f99ce62b-6977-47f9-82d2-494e20b80db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Best practices (Databricks link)\n",
    "\n",
    "Ref: https://www.databricks.com/discover/pages/optimize-data-workloads-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb2d8176-f05d-48b5-8f02-17c72502e554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2.0 Practical Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c478eb92-4202-47a3-a38b-361588a057d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2.1 One Problem, Many Solutions\n",
    "\n",
    "**Problem scenario**: Let's say we want to get the list of supplier names that have at least one invoice with a payment_status of 'Paid'. We aren't interested in the invoice details itself in the output.\n",
    "\n",
    "**df_serviceprovider**:\n",
    "\n",
    "| id | name |\n",
    "|----|------|\n",
    "| 201 | Service Provider 201 |\n",
    "| 202 | Service Provider 202 |\n",
    "| 203 | Service Provider 203 |\n",
    "| 204 | Service Provider 204 |\n",
    "\n",
    "**df_invoice**:\n",
    "\n",
    "| invoice_code | serviceprovider_id | invoice_amount | invoice_date | invoice_payment_status |\n",
    "|--------------|-------------|----------------|--------------|------------------------| \n",
    "| INV201001 | 201 | 10000.00 | 2022-01-01 | Paid |\n",
    "| INV201002 | 201 | 15000.00 | 2022-01-02 | Pending |\n",
    "| INV202001 | 202 | 7500.00 | 2022-01-01 | Paid |\n",
    "| INV203001 | 203 | 9000.00 | 2022-01-01 | Pending |\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "In this join scenario, there are at least four different ways to write the corresponding PySpark query. \n",
    "* Using Left-semi join\n",
    "* Using `isin()`\n",
    "* Using Inner join\n",
    "* Using Left outer join by filtering out unmatched rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4465db41-584f-4432-aca5-1f897b8420db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using Left-SEMI join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c8e364-706f-4684-9631-df6f2f6078a5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":238},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762555146692}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 5 (demo on One Problem, Many Solutions)\n",
    "df_sp = df_serviceprovider.alias(\"df_sp\")\n",
    "df_inv = df_invoice.alias(\"df_inv\")\n",
    "\n",
    "df_result = (\n",
    "    df_sp.join(\n",
    "        df_inv.filter(col(\"invoice_payment_status\") == \"Paid\"),\n",
    "        df_sp.id == df_inv.serviceprovider_id,\n",
    "        \"left_semi\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0582595a-66bc-4338-b0ac-a4a3e2fbff4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using ISIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ab80ad-fdce-44f3-8652-f120c326af16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 5 (demo on One Problem, Many Solutions)\n",
    "df_sp = df_serviceprovider.alias(\"df_sp\")\n",
    "df_inv = df_invoice.alias(\"df_inv\")\n",
    "\n",
    "# Get a list of qualified supplier Ids first\n",
    "sp_ids = [row.serviceprovider_id for row in df_inv\n",
    "    .filter(col(\"invoice_payment_status\") == \"Paid\")\n",
    "    .select(\"serviceprovider_id\")\n",
    "    .distinct()\n",
    "    .collect() # collect() could impact performance if the list is very long\n",
    "]\n",
    "\n",
    "df_result = df_sp.filter(col(\"id\").isin(sp_ids))\n",
    "\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cea5897a-4506-430c-b7b2-16d8c52ef97b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using INNER JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ce8212b-59bd-45bc-b871-9b48db40c5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 5 (demo on One Problem, Many Solutions)\n",
    "df_sp = df_serviceprovider.alias(\"df_sp\")\n",
    "df_inv = df_invoice.alias(\"df_inv\")\n",
    "\n",
    "df_result = (\n",
    "    df_sp.join(\n",
    "        df_inv.filter(col(\"invoice_payment_status\") == \"Paid\"),\n",
    "        df_sp.id == df_inv.serviceprovider_id,\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70e534b0-ff50-4275-a3dc-b621be9345e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using LEFT OUTER JOIN + filter out unmatched rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "554c60b2-ae9b-4983-b553-72d61381ef00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 5 (demo on One Problem, Many Solutions)\n",
    "df_sp = df_serviceprovider.alias(\"df_sp\")\n",
    "df_inv = df_invoice.alias(\"df_inv\")\n",
    "\n",
    "df_result = (\n",
    "    df_sp.join(\n",
    "        df_inv.filter(col(\"invoice_payment_status\") == \"Paid\"),\n",
    "        df_sp.id == df_inv.serviceprovider_id,\n",
    "        \"left_outer\"\n",
    "    )\n",
    "    .filter(col(\"serviceprovider_id\").isNotNull())\n",
    ")\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3e3a3e-da92-4ab1-a9ca-447a991833f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Key Takeaways: One Problem, Many Solutions\n",
    "\n",
    "We may be able to write a query in several different ways. Each approach may produce the same result, but the best choice depends on factors like performance, readability, and personal preference.\n",
    "\n",
    "So, I would start writing the query in an approach of my personal preference (in this example, left-semi join), and depending on how it performs, I would explore other options.\n",
    "\n",
    "Note: A given query performs depends on various factors including size of data etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57921664-88ec-4093-a1dd-b2eeb1fed49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2.2 Joining tables on a range of values\n",
    "\n",
    "**Join scenario**: Using the invoice_amount in the df_invoice table as below, join it on a range of values (lower_bound / upper_bound) in df_invoicecategory table, to determine the invoice category for each of the invoices.\n",
    "\n",
    "**df_invoice**:\n",
    "\n",
    "| invoice_code | supplier_id | invoice_amount | invoice_date | invoice_payment_status |\n",
    "|--------------|-------------|----------------|--------------|------------------------| \n",
    "| INV201001 | 201 | 10000.00 | 2022-01-01 | Paid |\n",
    "| INV201002 | 201 | 15000.00 | 2022-01-02 | Pending |\n",
    "| INV202001 | 202 | 7500.00 | 2022-01-01 | Paid |\n",
    "| INV203001 | 203 | 9000.00 | 2022-01-01 | Pending |\n",
    "\n",
    "**df_invoicecategory**:\n",
    "\n",
    "| category_description | lower_bound | upper_bound |\n",
    "|----------------------|-------------|-------------|\n",
    "| Invoice Category: 0-5K | 0.00 | 5000.00 |\n",
    "| Invoice Category: 5K-10K | 5000.01 | 10000.00 |\n",
    "| Invoice Category: 10K-20K | 10000.01 | 20000.00 |\n",
    "| Invoice Category: Over 20K | 20000.01 | 999999.00 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37149842-9209-47e6-b37f-e4ef9f0c2feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 5 (demo on One Problem, Many Solutions)\n",
    "df_inv_alias = df_invoice.alias(\"df_inv_alias\") \n",
    "df_invcat_alias = df_invoicecategory.alias(\"df_invcat_alias\")\n",
    "\n",
    "df_result = (\n",
    "    df_inv_alias\n",
    "    .join(df_invcat_alias, \n",
    "\n",
    "        #Joining on a range of values\n",
    "        (df_inv_alias.invoice_amount.between(df_invcat_alias.lower_bound,df_invcat_alias.upper_bound)),\n",
    "\n",
    "        \"leftouter\"\n",
    "   )\n",
    "    .select(\n",
    "    df_inv_alias.invoice_code.alias(\"invoice_code\"),\n",
    "    df_inv_alias.supplier_id.alias(\"supplier_id\"),\n",
    "    df_inv_alias.invoice_amount.alias(\"invoice_amount\"),\n",
    "    df_invcat_alias.category_description.alias(\"invoice_category\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb6a0dcb-c6e5-445c-a228-0066fc9f5504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2.3 Dealing with role playing dimensions\n",
    "\n",
    "**What's a role playing dimension?**\n",
    "\n",
    "Say, we have sales transaction records that have multiple key dates associated with it: Order Date, Ship Date, and Delivery Date.\n",
    "\n",
    "A fact table `FactSales` designed on the sales transaction data would have the below foreign keys to represent the three dates that we were talking about, apart from all other necessary fields:\n",
    "* OrderDateKey\n",
    "* ShipDateKey\n",
    "* DeliveryDateKey\n",
    "\n",
    "These three foreign keys reference the `DimDate` dimension, which is physically one dimension table.\n",
    "\n",
    "In other words, the fact table has multiple foreign keys that all point to the same dimension table.\n",
    "\n",
    "For this reason, dimension tables such as `DimDate` are called as Role-playing dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64b65d9b-ca6f-4f26-885b-cb58cabe0c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Scenario**: Say, we have:\n",
    "* Fact Table: `FactSales` (with foreign keys OrderDateKey, ShipDateKey, DeliveryDateKey).\n",
    "* Dimension Table: `DimDate` (with attributes like date, month, year, day of week).\n",
    "\n",
    "Role-Playing: The `DimDate` table is linked to the `FactSales` table three times, once for each date key, creating three independent views of the same date dimension.\n",
    "* DimDate as Order Date (linked to OrderDateKey)\n",
    "* DimDate as Ship Date (linked to ShipDateKey)\n",
    "* DimDate as Delivery Date (linked to DeliveryDateKey) \n",
    "\n",
    "For each of the transactions in the FactSales, retrieve transaction details as well as the Date and DateName for each of the OrderDates, ShipDates and DeliveryDates\n",
    "\n",
    "**df_factsales**:\n",
    "\n",
    "| transaction_id | OrderDateKey | ShipDateKey | DeliveryDateKey | ProductKey | CustomerKey | QuantitySold | UnitPrice |\n",
    "|----------------|--------------|-------------|-----------------|------------|-------------|--------------|-----------|\n",
    "| 1000001 | 20251001 | 20251002 | 20251003 | PKey001 | CKey301 | 10 | 100 |\n",
    "| 1000002 | 20251004 | 20251005 | 20251006 | PKey002 | CKey302 | 5 | 10 |\n",
    "| 1000003 | 20251006 | 20251007 | 20251008 | PKey003 | CKey303 | 3 | 150 |\n",
    "| 1000004 | 20251007 | 20251008 | 20251010 | PKey004 | CKey304 | 12 | 16 |\n",
    "\n",
    "**df_dimdate**:\n",
    "\n",
    "| DateKey | FullDate | DayOfWeek | DayName | Month | MonthName | Quarter | Year | IsHoliday | FiscalPeriod |\n",
    "|---------|----------|-----------|---------|-------|-----------|---------|------|-----------|--------------|\n",
    "| 20251001 | 2025-10-01 | 4 | Wednesday | 10 | October | 4 | 2025 | false | Q4-2025 |\n",
    "| 20251002 | 2025-10-02 | 5 | Thursday | 10 | October | 4 | 2025 | false | Q4-2025 |\n",
    "| 20251003 | 2025-10-03 | 6 | Friday | 10 | October | 4 | 2025 | false | Q4-2025 |\n",
    "| 20251004 | 2025-10-04 | 7 | Saturday | 10 | October | 4 | 2025 | true | Q4-2025 |\n",
    "| 20251005 | 2025-10-05 | 1 | Sunday | 10 | October | 4 | 2025 | true | Q4-2025 |\n",
    "| 20251006 | 2025-10-06 | 2 | Monday | 10 | October | 4 | 2025 | false | Q4-2025 |\n",
    "| 20251007 | 2025-10-07 | 3 | Tuesday | 10 | October | 4 | 2025 | false | Q4-2025 |\n",
    "| 20251008 | 2025-10-08 | 4 | Wednesday | 10 | October | 4 | 2025 | false | Q4-2025 |\n",
    "| 20251009 | 2025-10-09 | 5 | Thursday | 10 | October | 4 | 2025 | false | Q4-2025 |\n",
    "| 20251010 | 2025-10-10 | 6 | Friday | 10 | October | 4 | 2025 | false | Q4-2025 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f685ee63-240c-442e-abae-e5f272b0ff1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution outline**:\n",
    "\n",
    "Join the `FactSales` table to `DimDate` as three instances:\n",
    "* 1st instance of `DimDate` representing OrderDates\n",
    "* 2nd instance of `DimDate` representing ShipDates\n",
    "* 3rd instance of `DimDate` representing DeliveryDates\n",
    "\n",
    "Join these four dataframes, very similar to how you would do joins in a multi-table join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef803c5a-6d89-45d1-8653-f48f7c3a8e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Ref dataset: Example data 6 (demo on role-playing dimension)\n",
    "\n",
    "df_factsales_alias = df_factsales.alias(\"df_factsales_alias\") \n",
    "#Notice how the same df_dimdate is being instantiated into three separate instances: \n",
    "# df_orderdate_alias, df_shipdate_alias, df_deliverydate_alias\n",
    "df_orderdate_alias = df_dimdate.alias(\"df_orderdate_alias\")\n",
    "df_shipdate_alias = df_dimdate.alias(\"df_shipdate_alias\")\n",
    "df_deliverydate_alias = df_dimdate.alias(\"df_deliverydate_alias\")\n",
    "\n",
    "df_result = (\n",
    "    df_factsales_alias\n",
    "    #Join factsales to df_orderdate_alias\n",
    "    .join(df_orderdate_alias, \n",
    "\n",
    "        (df_factsales_alias.OrderDateKey == df_orderdate_alias.DateKey),\n",
    "\n",
    "        \"inner\"\n",
    "    )\n",
    "    #Join factsales to df_shipdate_alias\n",
    "    .join(df_shipdate_alias,\n",
    "          (df_factsales_alias.ShipDateKey == df_shipdate_alias.DateKey),\n",
    "        \"inner\"      \n",
    "    )\n",
    "    #Join factsales to df_deliverydate_alias\n",
    "    .join(df_deliverydate_alias,\n",
    "          (df_factsales_alias.DeliveryDateKey == df_deliverydate_alias.DateKey),\n",
    "        \"inner\"      \n",
    "    )\n",
    "    .select(\n",
    "        df_factsales_alias.transaction_id.alias(\"transaction_id\"),\n",
    "        df_orderdate_alias.FullDate.alias(\"OrderDate\"),\n",
    "        df_orderdate_alias.DayName.alias(\"OrderDay\"),\n",
    "        df_shipdate_alias.FullDate.alias(\"ShipDate\"),\n",
    "        df_shipdate_alias.DayName.alias(\"ShipDay\"),\n",
    "        df_deliverydate_alias.FullDate.alias(\"DeliveryDate\"),\n",
    "        df_deliverydate_alias.DayName.alias(\"DeliveryDay\"),\n",
    "        df_factsales_alias.ProductKey.alias(\"ProductKey\"),\n",
    "        df_factsales_alias.CustomerKey.alias(\"CustomerKey\"),\n",
    "        df_factsales_alias.QuantitySold.alias(\"QuantitySold\"),\n",
    "        df_factsales_alias.UnitPrice.alias(\"UnitPrice\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"transaction_id\").asc()) #For code brevity, prefer referencing aliased column names in sort() by using col()\n",
    "\n",
    ")\n",
    "\n",
    "df_result.display()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4797949245640627,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Joins - Part 3 - Common pitfalls and practical insights",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
