{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82086479-728f-4972-97b2-09aa7ccd8434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark joins - beyond the basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f0275d-2e12-4c1e-9195-57e2d1cd9546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Syntax - PySpark join\n",
    "\n",
    "`dataframe1.join(dataframe2, on=None, how=None)`\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "  *other* : Other dataframe, which is the RIGHT side of the join\n",
    "\n",
    "  *on* : Which column(s) to join ON?\n",
    "\n",
    "  *how* : Join type as string. Default INNER. \n",
    "\n",
    "**Returns:** Joined dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "710cf733-d38c-486b-ac25-909b2ed9d7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Focus of this notebook: How to join dataframes?\n",
    "\n",
    "The previous part 1 of the series focused on Join Types.\n",
    "\n",
    "Now, in this part 2, the primary focus will be on writing Join Conditions. That is, how to join dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28277500-8b71-4761-801a-8dd2ece73bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##ER Diagram: tpch database\n",
    "\n",
    "Note:  tpch is a database in samples catalog that's **available in Databricks free edition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71954193-1c7a-4cff-9113-a6c374ce709c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"./ERDiagram_Databricks_Samples_catalog_tpch_db.png\" alt=\"ERDiagram_Databricks_Samples_catalog_tpch_db.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32a35a94-5c95-438a-9a92-89b57722a73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Example data 1 (samples.tpch)\n",
    "\n",
    "Note: The **samples catalog** and the **tpch schema** are available by default in Databricks free edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb795b8-a140-4589-b1e6-6a1cbda4c484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customer = spark.read.table(\"samples.tpch.customer\")\n",
    "df_orders = spark.read.table(\"samples.tpch.orders\")\n",
    "df_lineitem = spark.read.table(\"samples.tpch.lineitem\")\n",
    "df_partsupp = spark.read.table(\"samples.tpch.partsupp\")\n",
    "df_part = spark.read.table(\"samples.tpch.part\")\n",
    "df_supplier = spark.read.table(\"samples.tpch.supplier\")\n",
    "\n",
    "# df_customer.show(5, truncate=False)\n",
    "# df_orders.show(5, truncate=False)\n",
    "# df_lineitem.show(5, truncate=False)\n",
    "# df_partsupp.show(5, truncate=False)\n",
    "# df_part.show(5, truncate=False)\n",
    "# df_supplier.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22874a27-36bc-41d8-8016-9e0623bb9252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Before you start writing PySpark joins\n",
    "\n",
    "Before we start writing PySpark joins, it's important to have clarity on a few key points:\n",
    "\n",
    "1. Which join type to use\n",
    "2. Which columns to join the dataframes on\n",
    "3. Choose a column access method\n",
    "4. Choose a dataframe aliasing approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be5f003-545b-45e3-a0c7-05f74e301e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###(i) Join types - my blog post\n",
    "\n",
    "Deciding on the join type is driven by the functional requirement. \n",
    "\n",
    "However, to have a clear understanding of various join types that Spark supports, I have explained it in detail in my blog-post on Medium.\n",
    "\n",
    "https://medium.com/@jpilli/write-pyspark-joins-like-a-pro-part-1-join-types-explained-a5a2f43678cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d33720a6-2643-476e-9b51-c9bacd5c07bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###INNER JOIN was used in the examples in here!\n",
    "\n",
    "Although most of the examples in this notebook used INNER JOIN as the join type, switching join type from one join to another is a matter of simply changing the join type string value in the join transformation, with the exception of cross join. In case of cross join, apart from specifying join type as \"cross\", you'd need to change join condition to None.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a0509c7-bbb2-46da-b7cc-b6951cbe8e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###(ii) Which columns to join dataframes on?\n",
    "\n",
    "This is driven by entity relationship of the tables that you want to join.\n",
    "\n",
    "For this demo, you can refer to the ER Diagram provided earlier in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "459350bd-ab02-4f0b-b525-b3d47f0c5e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###(iii) Column Access Methods - best practice recommendations\n",
    "\n",
    "A **hybrid approach** is recommended, as no single method covers all scenarios for accessing columns in a DataFrame.\n",
    "\n",
    "**Option A**\n",
    "\n",
    "* Use **col()** for most transformations, including when working with aliased/aggregated columns or column names containing special characters.\n",
    "* Use **string literals** only when:\n",
    "  - Specifying a new column name with `withColumn()`, or\n",
    "  - Renaming a column with `withColumnRenamed()`.\n",
    "\n",
    "**Option B**\n",
    "\n",
    "* Use **dot notation** for concise, readable code. It is familiar to Python users and provides a consistent coding style.\n",
    "* Use **col()** when working with aliased/aggregated columns or columns with special characters.\n",
    "* Use **string literals** only when:\n",
    "  - Specifying a new column name with `withColumn()`, or\n",
    "  - Renaming a column with `withColumnRenamed()`.\n",
    "\n",
    "**Either option works well**; choosing between them is a matter of what suits you best.\n",
    "\n",
    "Note: The best practice recommendation as above is an extract from my blog post: \n",
    "\n",
    "https://medium.com/@jpilli/pyspark-dataframes-which-column-access-method-should-you-use-best-practices-explained-f86c69d67fb8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8e545fa-3c78-48fd-a141-1f53505c9ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Column Access Methods used in these demos\n",
    "\n",
    "Most of the code examples in this notebook are available in:\n",
    "* Dot notation approach, as well as\n",
    "* col() function approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d01b6916-c1d1-47ae-ab9b-228b25d4073d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###(iv) Dataframe aliasing approaches\n",
    "\n",
    "* Approach 1: Alias dataframes before join transformation\n",
    "* Approach 2: Alias dataframes inline, in join transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75146b1c-a329-4c1b-b21f-744e155df589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Alias dataframes before performing a join\n",
    "\n",
    "When dataframes are aliased before the join transformation statement, then:\n",
    "* Both Dot notation and `col()` function work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df033ed4-1184-47d4-add8-946935fd3c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Tip: Ensure the variable name and string name passed to df.alias() method are exactly the same\n",
    "df_cust_aliased1 = df_customer.alias(\"df_cust_aliased1\") \n",
    "df_ord_aliased1 = df_orders.alias(\"df_ord_aliased1\")\n",
    "\n",
    "df_result_DFsAliasedBefore = (\n",
    "    df_cust_aliased1\n",
    "    .join(df_ord_aliased1, \n",
    "\n",
    "        #using dot notation : Works fine\n",
    "        (df_cust_aliased1.c_custkey == df_ord_aliased1.o_custkey),\n",
    "\n",
    "        #using col() : Works fine\n",
    "        # (col(\"df_cust_aliased1.c_custkey\") == col(\"df_ord_aliased1.o_custkey\")),  \n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    ")\n",
    "\n",
    "df_result_DFsAliasedBefore.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa3dcea4-f717-4b4f-9d29-71e0f101802a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Alias Dataframes inline while performing the join\n",
    "\n",
    "The **limitations** when dataframes being joined are aliased in-line:\n",
    "* Dot notation doesn't work. Only `col()` function works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79fdcff3-8e5d-4ffe-b72e-dce4581365a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_result_DFsAliasedInLine = (\n",
    "    df_customer.alias(\"df_cust_aliased\")\n",
    "    .join(df_orders.alias(\"df_ord_aliased\"), \n",
    "\n",
    "        #using dot notation : DOES NOT WORK\n",
    "        #(df_cust_aliased.c_custkey == df_ord_aliased.o_custkey),\n",
    "\n",
    "        #using col()\n",
    "        (col(\"df_cust_aliased.c_custkey\") == col(\"df_ord_aliased.o_custkey\")),  \n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    ")\n",
    "\n",
    "df_result_DFsAliasedInLine.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d47ecdd-5842-494f-9329-053ac40b6ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####What if I don't alias dataframes while joining them?\n",
    "\n",
    "**The limitations** when dataframes being joined aren't aliased:\n",
    "* Dot notation works fine. \n",
    "* `col()` function doesn't work. \n",
    "* cannot support creating multiple instances of the same dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b5f510b-b6db-46c1-aff6-4ade2a0c4a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_result_NoDFAliases = (\n",
    "    df_customer\n",
    "    .join(df_orders, \n",
    "\n",
    "        #using dot notation. \n",
    "        (df_customer.c_custkey == df_orders.o_custkey),\n",
    "\n",
    "        #using col() : DOES NOT WORK\n",
    "        # You need to alias your DataFrames before using col() with qualified column names in join conditions.\n",
    "        #(col(\"df_customer.c_custkey\") == col(\"df_orders.o_custkey\")),  \n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    ")\n",
    "\n",
    "df_result_NoDFAliases.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "280352d9-ea00-457c-baa4-c9ccad6cbe14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Key Takeaways - Aliasing Dataframes being joined\n",
    "\n",
    "To specify join condition in a transformation statement:\n",
    "\n",
    "* (*1st preferred*) Alias dataframes **before** performing a join:\n",
    "    - Benefit: you can use *either dot notation or `col()`* in join condition\n",
    "\n",
    "* (*2nd preferred*) Alias dataframes **inline** while performing a join:\n",
    "    - limitation: you can use `col()` but not dot notation in join condition\n",
    "\n",
    "* If we didn't alias dataframes while performing a join:\n",
    "    - limitation: you can use *dot notation* but not the `col()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc36ade4-36e3-4f83-88bf-079bf28aa40f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1.0 Single Column Join\n",
    "\n",
    "**Join scenario**: \n",
    "\n",
    "* Dataframes to join: *df_customer, df_orders*\n",
    "* Join condition: *df_customer.c_custkey = df_orders.o_custkey*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6251f17d-dc37-4b29-a474-c32f28a8d325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.1 Alias dataframes before performing the join\n",
    "\n",
    "Benefits:\n",
    "* Supports Dot notation as well as col() to access columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87cc0c97-bd84-4549-9114-ff8f020a32fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using Dot notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9867f2c-42f9-4bf8-a7c9-048b71c300ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Tip: Ensure the variable name and string name passed to df.alias() method are exactly the same\n",
    "df_cust_alias = df_customer.alias(\"df_cust_alias\") \n",
    "df_ord_alias = df_orders.alias(\"df_ord_alias\")\n",
    "\n",
    "df_result_scol_DfsAliasedB4_dotNotation = (\n",
    "    df_cust_alias\n",
    "    .join(df_ord_alias, \n",
    "\n",
    "        #using dot notation : Works fine\n",
    "        (df_cust_alias.c_custkey == df_ord_alias.o_custkey),\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    #Given we aliased dfs before the join transformation, you could use either dot notation or col() to access columns\n",
    "    .select(\n",
    "    df_cust_alias.c_custkey.alias(\"cust_key\"),\n",
    "    df_cust_alias.c_name.alias(\"cust_name\"),\n",
    "    df_ord_alias.o_orderkey.alias(\"order_key\"),\n",
    "    df_ord_alias.o_custkey.alias(\"cust_key_in_orders\"),\n",
    "    df_ord_alias.o_orderstatus.alias(\"order_status\"),\n",
    "    df_ord_alias.o_totalprice.alias(\"order_total_price\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"cust_key\").asc()) #For code brevity, prefer referencing aliased column names in sort() by using col()\n",
    "          \n",
    "    #To use dot notation in sort(), you'll need to reference original column name rather than aliased name\n",
    "    #.sort(df_ord_alias.o_custkey.asc()) \n",
    ")\n",
    "\n",
    "df_result_scol_DfsAliasedB4_dotNotation.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "185056f3-cf36-433d-868c-5b1e11929539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using col() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab5e3c16-d94d-44fa-9c32-5785a5142d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Tip: Ensure the variable name and string name passed to df.alias() method are exactly the same\n",
    "df_cust_alias = df_customer.alias(\"df_cust_alias\") \n",
    "df_ord_alias = df_orders.alias(\"df_ord_alias\")\n",
    "\n",
    "df_result_scol_DfsAliasedB4_col = (\n",
    "    df_cust_alias\n",
    "    .join(df_ord_alias, \n",
    "\n",
    "        #using col() : Works fine\n",
    "        (col(\"df_cust_alias.c_custkey\") == col(\"df_ord_alias.o_custkey\")),  \n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    #Given we aliased dfs before the join transformation, you could use either dot notation or col() to access columns\n",
    "    .select(\n",
    "    col(\"df_cust_alias.c_custkey\").alias(\"cust_key\"),\n",
    "    col(\"df_cust_alias.c_name\").alias(\"cust_name\"),\n",
    "    col(\"df_ord_alias.o_orderkey\").alias(\"order_key\"),\n",
    "    col(\"df_ord_alias.o_custkey\").alias(\"cust_key_in_orders\"),\n",
    "    col(\"df_ord_alias.o_orderstatus\").alias(\"order_status\"),\n",
    "    col(\"df_ord_alias.o_totalprice\").alias(\"order_total_price\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"cust_key_in_orders\").asc()) #With col(), you get to reference aliased column names in sort()\n",
    ")\n",
    "\n",
    "df_result_scol_DfsAliasedB4_col.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba759f7b-c18f-4e5b-8977-563192576114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.2 Alias Dataframes inline while performing the join\n",
    "\n",
    "Benefits:\n",
    "* Supports col() to access columns\n",
    "\n",
    "Limitations:\n",
    "* Doesn't support dot notation to access columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1462e927-bea8-4487-b829-f6240df0a2a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using Dot notation - NOT SUPPORTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64156111-20a7-4bf3-ab49-1bb4f60e734c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using col() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac9748f-b2f6-48e1-ae63-c1bda14de5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_result_scol_DfsAliasedInLine_col = (\n",
    "    df_customer.alias(\"df_cust_alias\")\n",
    "    .join(df_orders.alias(\"df_ord_alias\"), \n",
    "\n",
    "        #using col() : Works fine\n",
    "        (col(\"df_cust_alias.c_custkey\") == col(\"df_ord_alias.o_custkey\")),  \n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    # IMPORTANT: Given we aliased dataframes inline, only col() function is supported to access columns.\n",
    "    .select(\n",
    "    col(\"df_cust_alias.c_custkey\").alias(\"cust_key\"),\n",
    "    col(\"df_cust_alias.c_name\").alias(\"cust_name\"),\n",
    "    col(\"df_ord_alias.o_orderkey\").alias(\"order_key\"),\n",
    "    col(\"df_ord_alias.o_custkey\").alias(\"cust_key_in_orders\"),\n",
    "    col(\"df_ord_alias.o_orderstatus\").alias(\"order_status\"),\n",
    "    col(\"df_ord_alias.o_totalprice\").alias(\"order_total_price\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"cust_key_in_orders\").asc()) #With col(), you get to reference aliased column names in sort()\n",
    ")\n",
    "\n",
    "df_result_scol_DfsAliasedInLine_col.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3be1f2a9-b6ff-4b7c-8430-e38454899695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2.0 Multi-Column Joins\n",
    "\n",
    "i.e Join condition on two or more columns\n",
    "\n",
    "**Join scenario**: \n",
    "\n",
    "* Dataframes to join: *df_lineitem, df_partsupp*\n",
    "* Join condition: \n",
    "    - *df_lineitem.l_partkey = df_partsupp.ps_partkey*\n",
    "    - *df_lineitem.l_suppkey = df_partsupp.ps_suppkey*\n",
    "\n",
    "Tip 1: It is the same syntax whether your join condition is on two columns or many columns.\n",
    "\n",
    "Tip 2: While specifying joins on multiple columns, \n",
    "* enclose each column join expression in a separate parentheses ()\n",
    "* optionally, enclose all column join expressions in a List[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51ef0333-2876-4edf-b471-1c2e8e149be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2.1 Alias dataframes before performing the join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e58be64c-e988-4188-b716-9e3b1eebb345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using Dot notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c095cca-c6e6-42d8-b2f1-372bf6703d2c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761020151220}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Tip: Ensure the variable name and string name passed to df.alias() method are exactly the same\n",
    "df_lineitem_alias = df_lineitem.alias(\"df_lineitem_alias\") \n",
    "df_partsupp_alias = df_partsupp.alias(\"df_partsupp_alias\")\n",
    "\n",
    "df_result_mcols_DfsAliasedB4_dot = (\n",
    "    df_lineitem_alias\n",
    "    .join(df_partsupp_alias, \n",
    "\n",
    "        #using dot notation\n",
    "        [(df_lineitem_alias.l_partkey == df_partsupp_alias.ps_partkey) &\n",
    "        (df_lineitem_alias.l_suppkey == df_partsupp_alias.ps_suppkey)],\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    #Given we aliased dfs before the join transformation, you could use either dot notation or col() to access columns\n",
    "    .select(\n",
    "    df_lineitem_alias.l_orderkey.alias(\"orderkey\"),\n",
    "    df_lineitem_alias.l_partkey.alias(\"partkey_InLI\"),\n",
    "    df_lineitem_alias.l_suppkey.alias(\"suppkey_InLI\"),\n",
    "    df_lineitem_alias.l_linenumber.alias(\"linenumber\"),\n",
    "    df_lineitem_alias.l_quantity.alias(\"quantity\"),\n",
    "    df_partsupp_alias.ps_partkey.alias(\"partkey_inPartSupp\"),\n",
    "    df_partsupp_alias.ps_suppkey.alias(\"suppkey_inPartSupp\"),\n",
    "    df_partsupp_alias.ps_availqty.alias(\"availqty\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"orderkey\").asc(), col(\"linenumber\").asc()) #For code brevity, prefer referencing aliased column names in sort() by using col()\n",
    "\n",
    "    #To use dot notation in sort(), you'll need to reference original column name rather than aliased name\n",
    "    #.sort(df_lineitem_alias.l_orderkey.asc(), df_lineitem_alias.l_linenumber.asc()) \n",
    ")\n",
    "\n",
    "df_result_mcols_DfsAliasedB4_dot.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e1777e4-8eae-49c4-b3b0-464ae961d0cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using col() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5908c3f2-cdb2-4d57-82d5-1e05b5b8d11c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761021266092}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Tip: Ensure the variable name and string name passed to df.alias() method are exactly the same\n",
    "df_lineitem_alias = df_lineitem.alias(\"df_lineitem_alias\") \n",
    "df_partsupp_alias = df_partsupp.alias(\"df_partsupp_alias\")\n",
    "\n",
    "df_result_mcols_DfsAliasedB4_col = (\n",
    "    df_lineitem_alias\n",
    "    .join(df_partsupp_alias, \n",
    "\n",
    "        #using col()\n",
    "        [(col(\"df_lineitem_alias.l_partkey\") == col(\"df_partsupp_alias.ps_partkey\")) &\n",
    "        (col(\"df_lineitem_alias.l_suppkey\") == col(\"df_partsupp_alias.ps_suppkey\"))],\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    #Given we aliased dfs before the join transformation, you could use either dot notation or col() to access columns\n",
    "    .select(\n",
    "    col(\"df_lineitem_alias.l_orderkey\").alias(\"orderkey\"),\n",
    "    col(\"df_lineitem_alias.l_partkey\").alias(\"partkey_InLI\"),\n",
    "    col(\"df_lineitem_alias.l_suppkey\").alias(\"suppkey_InLI\"),\n",
    "    col(\"df_lineitem_alias.l_linenumber\").alias(\"linenumber\"),\n",
    "    col(\"df_lineitem_alias.l_quantity\").alias(\"quantity\"),\n",
    "\n",
    "    col(\"df_partsupp_alias.ps_partkey\").alias(\"partkey_inPartSupp\"),\n",
    "    col(\"df_partsupp_alias.ps_suppkey\").alias(\"suppkey_inPartSupp\"),\n",
    "    col(\"df_partsupp_alias.ps_availqty\").alias(\"availqty\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"orderkey\").asc(), col(\"linenumber\").asc()) #With col(), you get to reference aliased column names in sort()\n",
    ")\n",
    "\n",
    "df_result_mcols_DfsAliasedB4_col.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48026e90-0b0c-4384-8610-8ac3b14134c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2.2 Alias Dataframes inline while performing the join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72968832-5cf5-4dec-b5fd-a08bd1d9974d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using Dot notation - NOT SUPPORTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f97be07-0dcf-4df1-bcd4-ee4b13c2d56f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using col() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "118a7438-05ca-4f2f-a781-d33574525cda",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761021339987}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_result_mcols_DfsAliasedInLine_col = (\n",
    "    df_lineitem.alias(\"df_lineitem_alias\")\n",
    "    .join(df_partsupp.alias(\"df_partsupp_alias\"), \n",
    "\n",
    "        #using col()\n",
    "        [(col(\"df_lineitem_alias.l_partkey\") == col(\"df_partsupp_alias.ps_partkey\")) &\n",
    "        (col(\"df_lineitem_alias.l_suppkey\") == col(\"df_partsupp_alias.ps_suppkey\"))],\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    # IMPORTANT: Given we aliased dataframes inline, only col() function is supported to access columns.\n",
    "    .select(\n",
    "    col(\"df_lineitem_alias.l_orderkey\").alias(\"orderkey\"),\n",
    "    col(\"df_lineitem_alias.l_partkey\").alias(\"partkey_InLI\"),\n",
    "    col(\"df_lineitem_alias.l_suppkey\").alias(\"suppkey_InLI\"),\n",
    "    col(\"df_lineitem_alias.l_linenumber\").alias(\"linenumber\"),\n",
    "    col(\"df_lineitem_alias.l_quantity\").alias(\"quantity\"),\n",
    "\n",
    "    col(\"df_partsupp_alias.ps_partkey\").alias(\"partkey_inPartSupp\"),\n",
    "    col(\"df_partsupp_alias.ps_suppkey\").alias(\"suppkey_inPartSupp\"),\n",
    "    col(\"df_partsupp_alias.ps_availqty\").alias(\"availqty\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"orderkey\").asc(), col(\"linenumber\").asc()) #With col(), you get to reference aliased column names in sort()\n",
    ")\n",
    "\n",
    "df_result_mcols_DfsAliasedInLine_col.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ba011af-05d6-4a6c-aa7d-7b9b88dffe0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3.0 Multi-table Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76629da1-760f-46fb-ae84-5cc41cc425e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Join scenario**: \n",
    "\n",
    "* Dataframes to join: *df_lineitem, df_partsupp, df_part, df_supplier*\n",
    "* Join condition: \n",
    "    - *df_lineitem.l_partkey = df_partsupp.ps_partkey*\n",
    "    - *df_lineitem.l_suppkey = df_partsupp.ps_suppkey*\n",
    "    - *df_partsupp.ps_partkey = df_part.p_partkey*\n",
    "    - *df_partsupp.ps_suppkey = df_supplier.s_suppkey*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2d9e7e1-ec08-4d32-9f54-fb196e6c6ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.1 Alias dataframes before performing the join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a9c4018-76b6-4ce3-b68f-0eb8dccf0e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using Dot notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e68e53c-bc08-4e0f-8a4d-e20ca33e3acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Tip: Ensure the variable name and string name passed to df.alias() method are exactly the same\n",
    "df_lineitem_alias = df_lineitem.alias(\"df_lineitem_alias\") \n",
    "df_partsupp_alias = df_partsupp.alias(\"df_partsupp_alias\")\n",
    "df_part_alias = df_part.alias(\"df_part_alias\")\n",
    "df_supplier_alias = df_supplier.alias(\"df_supplier_alias\")\n",
    "\n",
    "df_result_mdfs_DfsAliasedB4_dotNotation = (\n",
    "    df_lineitem_alias\n",
    "    #Join LineItems with PartSupplier table\n",
    "    .join(df_partsupp_alias, \n",
    "\n",
    "        [(df_lineitem_alias.l_partkey == df_partsupp_alias.ps_partkey) &\n",
    "        (df_lineitem_alias.l_suppkey == df_partsupp_alias.ps_suppkey)],\n",
    "\n",
    "        \"inner\"\n",
    "    )\n",
    "    #Join PartSupplier table to Parts table\n",
    "    .join(df_part_alias,\n",
    "          (df_partsupp_alias.ps_partkey == df_part_alias.p_partkey),\n",
    "        \"inner\"      \n",
    "    )\n",
    "    #Join PartSupplier table to Supplier table\n",
    "    .join(df_supplier_alias,\n",
    "          (df_partsupp_alias.ps_suppkey == df_supplier_alias.s_suppkey),\n",
    "        \"inner\"      \n",
    "    )\n",
    "        \n",
    "    #Given we aliased dfs before the join transformation, you could use either dot notation or col() to access columns\n",
    "    .select(\n",
    "    df_lineitem_alias.l_orderkey.alias(\"orderkey\"),\n",
    "    df_lineitem_alias.l_partkey.alias(\"partkey_InLI\"),\n",
    "    df_lineitem_alias.l_suppkey.alias(\"suppkey_InLI\"),\n",
    "    df_lineitem_alias.l_linenumber.alias(\"linenumber\"),\n",
    "    df_lineitem_alias.l_quantity.alias(\"quantity\"),\n",
    "\n",
    "    df_partsupp_alias.ps_partkey.alias(\"partkey_inPartSupp\"),\n",
    "    df_partsupp_alias.ps_suppkey.alias(\"suppkey_inPartSupp\"),\n",
    "    df_partsupp_alias.ps_availqty.alias(\"availqty\"),\n",
    "    df_partsupp_alias.ps_supplycost.alias(\"supplycost\"),\n",
    "\n",
    "    df_part_alias.p_partkey.alias(\"partkey_inPart\"),\n",
    "    df_part_alias.p_name.alias(\"partname\"),\n",
    "\n",
    "    df_supplier_alias.s_suppkey.alias(\"suppkey_inSupplier\"),\n",
    "    df_supplier_alias.s_name.alias(\"suppliername\")    \n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"orderkey\").asc(), col(\"linenumber\").asc()) #For code brevity, prefer referencing aliased column names in sort() by using col()\n",
    "\n",
    "    #To use dot notation in sort(), you'll need to reference original column name rather than aliased name\n",
    "    #.sort(df_lineitem_alias.l_orderkey.asc(), df_lineitem_alias.l_linenumber.asc()) \n",
    ")\n",
    "\n",
    "df_result_mdfs_DfsAliasedB4_dotNotation.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a84bf2ee-ecf9-4200-b477-3fe09879ca49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using col() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccae232-ad6d-4d0f-9522-61abbfb06284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Tip: Ensure the variable name and string name passed to df.alias() method are exactly the same\n",
    "df_lineitem_alias = df_lineitem.alias(\"df_lineitem_alias\") \n",
    "df_partsupp_alias = df_partsupp.alias(\"df_partsupp_alias\")\n",
    "df_part_alias = df_part.alias(\"df_part_alias\")\n",
    "df_supplier_alias = df_supplier.alias(\"df_supplier_alias\")\n",
    "\n",
    "df_result_mdfs_DfsAliasedB4_col = (\n",
    "    df_lineitem_alias\n",
    "    #Join LineItems with PartSupplier table\n",
    "    .join(\n",
    "        df_partsupp_alias,\n",
    "        (col(\"df_lineitem_alias.l_partkey\") == col(\"df_partsupp_alias.ps_partkey\")) &\n",
    "        (col(\"df_lineitem_alias.l_suppkey\") == col(\"df_partsupp_alias.ps_suppkey\")),\n",
    "        \"inner\"\n",
    "    )\n",
    "    #Join PartSupplier table to Parts table\n",
    "    .join(\n",
    "        df_part_alias,\n",
    "        col(\"df_partsupp_alias.ps_partkey\") == col(\"df_part_alias.p_partkey\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    #Join PartSupplier table to Supplier table\n",
    "    .join(\n",
    "        df_supplier_alias,\n",
    "        col(\"df_partsupp_alias.ps_suppkey\") == col(\"df_supplier_alias.s_suppkey\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"df_lineitem_alias.l_orderkey\").alias(\"orderkey\"),\n",
    "        col(\"df_lineitem_alias.l_partkey\").alias(\"partkey_InLI\"),\n",
    "        col(\"df_lineitem_alias.l_suppkey\").alias(\"suppkey_InLI\"),\n",
    "        col(\"df_lineitem_alias.l_linenumber\").alias(\"linenumber\"),\n",
    "        col(\"df_lineitem_alias.l_quantity\").alias(\"quantity\"),\n",
    "        col(\"df_partsupp_alias.ps_partkey\").alias(\"partkey_inPartSupp\"),\n",
    "        col(\"df_partsupp_alias.ps_suppkey\").alias(\"suppkey_inPartSupp\"),\n",
    "        col(\"df_partsupp_alias.ps_availqty\").alias(\"availqty\"),\n",
    "        col(\"df_part_alias.p_partkey\").alias(\"partkey_inPart\"),\n",
    "        col(\"df_part_alias.p_name\").alias(\"partname\"),\n",
    "        col(\"df_supplier_alias.s_suppkey\").alias(\"suppkey_inSupplier\"),\n",
    "        col(\"df_supplier_alias.s_name\").alias(\"suppliername\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"orderkey\").asc(), col(\"linenumber\").asc()) \n",
    "\n",
    ")\n",
    "\n",
    "df_result_mdfs_DfsAliasedB4_col.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e7a9000-6487-455e-8797-fafb9408e631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.2 Alias Dataframes inline while performing the join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5b1736-6b44-4e2f-9221-815082b48525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using Dot notation - NOT SUPPORTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a6a48b3-df70-4ef3-bc7d-feb0f443005c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Using col() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc934342-f04e-42cd-9bae-d6cd0eaf599d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_resut_mdfs_DfsAliasedInLine_col = (\n",
    "    df_lineitem.alias(\"df_lineitem_alias\")\n",
    "    #Join LineItems with PartSupplier table\n",
    "    .join(\n",
    "        df_partsupp.alias(\"df_partsupp_alias\"),\n",
    "        (col(\"df_lineitem_alias.l_partkey\") == col(\"df_partsupp_alias.ps_partkey\")) &\n",
    "        (col(\"df_lineitem_alias.l_suppkey\") == col(\"df_partsupp_alias.ps_suppkey\")),\n",
    "        \"inner\"\n",
    "    )\n",
    "    #Join PartSupplier table to Parts table    \n",
    "    .join(\n",
    "        df_part.alias(\"df_part_alias\"),\n",
    "        col(\"df_partsupp_alias.ps_partkey\") == col(\"df_part_alias.p_partkey\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    #Join PartSupplier table to Supplier table\n",
    "    .join(\n",
    "        df_supplier.alias(\"df_supplier_alias\"),\n",
    "        col(\"df_partsupp_alias.ps_suppkey\") == col(\"df_supplier_alias.s_suppkey\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"df_lineitem_alias.l_orderkey\").alias(\"orderkey\"),\n",
    "        col(\"df_lineitem_alias.l_partkey\").alias(\"partkey_InLI\"),\n",
    "        col(\"df_lineitem_alias.l_suppkey\").alias(\"suppkey_InLI\"),\n",
    "        col(\"df_lineitem_alias.l_linenumber\").alias(\"linenumber\"),\n",
    "        col(\"df_lineitem_alias.l_quantity\").alias(\"quantity\"),\n",
    "        col(\"df_partsupp_alias.ps_partkey\").alias(\"partkey_inPartSupp\"),\n",
    "        col(\"df_partsupp_alias.ps_suppkey\").alias(\"suppkey_inPartSupp\"),\n",
    "        col(\"df_partsupp_alias.ps_availqty\").alias(\"availqty\"),\n",
    "        col(\"df_part_alias.p_partkey\").alias(\"partkey_inPart\"),\n",
    "        col(\"df_part_alias.p_name\").alias(\"partname\"),\n",
    "        col(\"df_supplier_alias.s_suppkey\").alias(\"suppkey_inSupplier\"),\n",
    "        col(\"df_supplier_alias.s_name\").alias(\"suppliername\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"orderkey\").asc(), col(\"linenumber\").asc()) \n",
    "\n",
    ")\n",
    "\n",
    "df_resut_mdfs_DfsAliasedInLine_col.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39bbd0a2-e046-427e-9374-446cbb43bd9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4.0 Self-join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79a721c4-70a0-4e76-b2cf-ed43dda787c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Example data 2 (self-join)\n",
    "\n",
    "Join the same dataframe to itself.\n",
    "\n",
    "**Join scenario**: \n",
    "\n",
    "* Dataframes to join: *df_employee (instance 1), df_employee (instance 2)*\n",
    "* Join condition: *df_instance1.manager_id = df_instance2.emp_id*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f404c15c-721a-4e10-97cc-9f209fb331ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Example data in *df_employee* looks as below:\n",
    "\n",
    "| emp_id | emp_name | job_title | manager_id |\n",
    "|--------|----------|-----------|------------|\n",
    "|101|David|CEO|None|\n",
    "|102|Kevin|GM|101|\n",
    "|103|Lisa|EM|102|\n",
    "|104|Mike|PM|103|\n",
    "|105|Jack|Data Engineer|104|\n",
    "|106|Melissa|Data Engineer|104|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcea4026-41b2-4fe6-8fe3-f10f3a4f687f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Load *df_employee* dataframe with example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf7e1e2-64ef-47a7-9455-f0da9809bd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_data = (\n",
    "    (101,\"David\",\"CEO\",None),\n",
    "    (102,\"Kevin\",\"GM\",101),\n",
    "    (103,\"Lisa\",\"EM\",102),\n",
    "    (104,\"Mike\",\"PM\",103),\n",
    "    (105,\"Jack\",\"Data Engineer\",104),\n",
    "    (106,\"Melissa\",\"Data Engineer\",104)\n",
    "  )\n",
    "employee_schema = \"emp_id: int, emp_name: string, job_title: string, manager_id: int\"\n",
    "\n",
    "df_employee = spark.createDataFrame(data = employee_data, schema = employee_schema)\n",
    "\n",
    "df_employee.select(\"*\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5096fdc-a0ff-4251-8d92-ba9df6753aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Using dot notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9fc62e7-f620-4d9b-aa29-a7e9745c3abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "#Tip: Ensure the variable name and string name passed to alias() method are exactly the same\n",
    "df_emp = df_employee.alias(\"df_emp\")\n",
    "df_mgr = df_employee.alias(\"df_mgr\")\n",
    "\n",
    "df_result_selfjoin = (\n",
    "    df_emp\n",
    "    .join(\n",
    "        df_mgr,\n",
    "        df_emp.manager_id == df_mgr.emp_id,\n",
    "        \"left_outer\" #Tip: Use left_outer join to be able to include employees that don't have a manager e.g. CEO\n",
    "    )\n",
    "    .select(\n",
    "        df_emp.emp_id.alias(\"Employee_Id\"),\n",
    "        df_emp.emp_name.alias(\"Employee_Name\"),\n",
    "        df_emp.job_title.alias(\"Employee_Job_Title\"),\n",
    "        df_mgr.emp_id.alias(\"Manager_Id\"),\n",
    "        df_mgr.emp_name.alias(\"Manager_Name\"),\n",
    "        df_mgr.job_title.alias(\"Manager_Job_Title\")\n",
    "    )\n",
    "    #Tip: Avoid sort() unless required\n",
    "    .sort(col(\"Employee_Id\").asc()) #For code brevity, prefer referencing aliased column names in sort() by using col()\n",
    ")\n",
    "\n",
    "df_result_selfjoin.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8270077-c20a-4d02-90ca-fd44d943b4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Using col() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6f7435-b8e8-4466-b398-738ae56b15d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "#Tip: Ensure the variable name and string name passed to alias() method are exactly the same \n",
    "df_emp = df_employee.alias(\"df_emp\")\n",
    "df_mgr = df_employee.alias(\"df_mgr\")\n",
    "\n",
    "df_result_selfjoin = (\n",
    "    df_emp\n",
    "    .join(\n",
    "        df_mgr,\n",
    "        col(\"df_emp.manager_id\") == col(\"df_mgr.emp_id\"),\n",
    "        \"left_outer\" #Tip: Use left_outer join to be able to include employees that don't have a manager e.g. CEO\n",
    "    )\n",
    "    .select(\n",
    "        col(\"df_emp.emp_id\").alias(\"Employee_Id\"),\n",
    "        col(\"df_emp.emp_name\").alias(\"Employee_Name\"),\n",
    "        col(\"df_emp.job_title\").alias(\"Employee_Job_Title\"),\n",
    "        col(\"df_mgr.emp_id\").alias(\"Manager_Id\"),\n",
    "        col(\"df_mgr.emp_name\").alias(\"Manager_Name\"),\n",
    "        col(\"df_mgr.job_title\").alias(\"Manager_Job_Title\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result_selfjoin.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "717c9455-1c6d-4710-918c-dd402dc7f4a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5.0 Using string literals as column names to join dataframes\n",
    "\n",
    "**Pre-requisites / Limitations**:\n",
    "* The column names used in the join condition must be identical in both the dataframes\n",
    "\n",
    "**Join scenario**: \n",
    "\n",
    "* Dataframes to join: *df_mockup_orders, df_mockup_lineitems, df_mockup_partsupp*\n",
    "* Join condition: \n",
    "    - *df_mockup_orders.orderkey = df_mockup_lineitems.orderkey*\n",
    "    - *df_mockup_lineitems.partkey = df_mockup_partsupp.partkey*\n",
    "    - *df_mockup_lineitems.suppkey = df_mockup_partsupp.suppkey*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73416eae-ca1d-44f6-9fd0-268a98900819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Example data 3 (string literals as joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e86265-9447-4a41-9561-842dd696fffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "#1. df_mockup_orders\n",
    "df_mockup_orders = spark.createDataFrame(\n",
    "    [\n",
    "        (1001,21,'Open', Decimal('45.00')),\n",
    "        (1002,22,'Open', Decimal('60.00')),\n",
    "        (1003,23,'Completed', Decimal('70.00')),\n",
    "        (1004,24,'Completed', Decimal('90.00')),\n",
    "        (1005,25,'Pending', Decimal('77.55')) \n",
    "    ],\n",
    "    \"orderkey: bigint, custkey: bigint, orderstatus: string, totalprice: decimal(18,2)\"\n",
    ")\n",
    "\n",
    "df_mockup_orders.display()\n",
    "\n",
    "#2. df_mockup_lineitems\n",
    "df_mockup_lineitems = spark.createDataFrame(\n",
    "    [\n",
    "        (1001,301,881,1,2),\n",
    "        (1002,311,881,1,3),\n",
    "        (1003,315,881,1,5),\n",
    "        (1004,321,881,1,2),\n",
    "        (1005,324,881,1,6)\n",
    "    ],\n",
    "    \"orderkey: bigint, partkey: bigint, suppkey: bigint, linenumber: int, quantity: int\"\n",
    ")\n",
    "df_mockup_lineitems.display()\n",
    "\n",
    "#3. df_mockup_partsupp\n",
    "df_mockup_partsupp = spark.createDataFrame(\n",
    "    [\n",
    "        (301,881,100),\n",
    "        (311,881,100),\n",
    "        (315,881,100),\n",
    "        (321,881,100),\n",
    "        (324,881,100) \n",
    "    ],\n",
    "    \"partkey: bigint, suppkey: bigint, availqty: int\"\n",
    ")\n",
    "\n",
    "df_mockup_partsupp.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "362f2471-ef69-4c12-bf14-eec2fbc3a2ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "###Single column join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162cc22f-a859-4523-a399-9e3189bc8ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_stringjoin_singlecol = (\n",
    "  df_mockup_orders\n",
    "  .join(df_mockup_lineitems,\n",
    "        \"orderkey\", #single column - join condition\n",
    "        \"inner\" #Default: INNER\n",
    "  )\n",
    "  .select(df_mockup_orders.orderkey, df_mockup_orders.custkey, df_mockup_orders.orderstatus, df_mockup_orders.totalprice, df_mockup_lineitems.linenumber, df_mockup_lineitems.quantity)\n",
    "  \n",
    "  #Tip: Avoid sort() unless required\n",
    "  .sort(\"orderkey\",\"linenumber\")\n",
    ")\n",
    "\n",
    "df_stringjoin_singlecol.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f701e2-da43-48db-915c-75422e38ca4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Multi-table join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d6d33ba-9dcb-4c5a-96d3-4274846a2d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_stringjoin_multicol = (\n",
    "  df_mockup_orders\n",
    "  .join(df_mockup_lineitems,\n",
    "        \"orderkey\", #single column - join condition\n",
    "        \"inner\" #Default: INNER\n",
    "  )\n",
    "  .join(df_mockup_partsupp,\n",
    "        [\"partkey\",\"suppkey\"], #Multi-colum join: List[] of column names as join condition\n",
    "        \"inner\"\n",
    "  )  \n",
    "  .select(df_mockup_orders.orderkey, df_mockup_orders.custkey, df_mockup_orders.orderstatus, df_mockup_orders.totalprice, df_mockup_lineitems.linenumber, df_mockup_lineitems.quantity, df_mockup_partsupp.availqty)\n",
    "\n",
    "  #Tip: Avoid sort() unless required\n",
    "  .sort(\"orderkey\",\"linenumber\")\n",
    ")\n",
    "\n",
    "df_stringjoin_multicol.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4f41a6-e94c-4c6f-bf06-a6c10a7b8559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6.0 Tips and tricks\n",
    "\n",
    "Last but not least, here are some useful tips and tricks to help you write PySpark joins more effectively.\n",
    "\n",
    "Note: Explanation for each of the tips follows the illustration as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38356810-19eb-40f2-b469-b2b11e7e6558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"./PySpark Joins - Tips.png\" alt=\"PySpark Joins - Tips.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40fbc0c5-ac6d-4b84-846b-8d614617b7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 1: If aliasing dataframes before performing a join, ensure the variable name and string passed as arg to df.alias() method are identical\n",
    "\n",
    "The benefit and reason behind it:\n",
    "* Benefit: It gives you flexibility in using the same aliased dataframe name regardless of whether you use dot notation or `col()` function in a join transformation.\n",
    "* Reason: In the join transformation, \n",
    "    - dot notation references variable that `df.alias()` method returned.\n",
    "    - where as, `col()` function references the string argument that was passed to `df.alias()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cbabd73-51bc-4f2f-b583-da83d300e4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Tip: Notice here that the string argument that was passed to df.alias() was identical to the variable that the df.alias() method returned\n",
    "df_lineitem_aliased1 = df_lineitem.alias(\"df_lineitem_aliased1\") \n",
    "df_partsupp_aliased1 = df_partsupp.alias(\"df_partsupp_aliased1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64dbfc3b-56f3-4f8b-8de0-5a4553c44c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 2: Prefer parentheses () to support multi-line transformation\n",
    "\n",
    "Prefer to enclose the entire join transformation on the right-hand side in parentheses() while assigning it to a new dataframe.\n",
    "\n",
    "For example, in the trimmed-down version of the code as below, the join transformation on the right hand side of the assignment operator was enclosed in opening parenthesis on line 9 and closing parenthesis on line 24.\n",
    "\n",
    "*Tip*: the opening parenthesis should start on the same line as the assignment operator (=) in assigning the transformation to a new dataframe as in line 9 rather than inserting opening parenthesis in the next line of code.\n",
    "\n",
    "**Benefits**:\n",
    "* Eliminates the need to use a backslash ( \\ ) at the end of each line in a multi-line transformation\n",
    "* Multi-line statement improves readability, especially in a complex transformation\n",
    "* Supports in-line comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41115720-017b-4297-b009-11b5de4cfdf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_lineitem_aliased1 = df_lineitem.alias(\"df_lineitem_aliased1\") \n",
    "df_partsupp_aliased1 = df_partsupp.alias(\"df_partsupp_aliased1\")\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "#To keep it brief, I’m showing a trimmed-down version of the code.\n",
    "#-----------------------------------------------------------------\n",
    "df_tips = (\n",
    "    df_lineitem_aliased1\n",
    "    .join(df_partsupp_aliased1, \n",
    "\n",
    "        #using dot notation\n",
    "        [(df_lineitem_aliased1.l_partkey == df_partsupp_aliased1.ps_partkey) &\n",
    "        (df_lineitem_aliased1.l_suppkey == df_partsupp_aliased1.ps_suppkey)],\n",
    "\n",
    "        \"inner\"\n",
    "   )#.select( #Tip: You could write .select() in this line itself or in the next line\n",
    "    .select(\n",
    "    df_lineitem_aliased1.l_orderkey.alias(\"orderkey\"),\n",
    "    df_lineitem_aliased1.l_partkey.alias(\"partkey_InLI\")\n",
    "    )\n",
    "    .sort(col(\"orderkey\").asc())\n",
    ")\n",
    "\n",
    "df_tips.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a59fa3c9-c824-410e-b6b7-b48552ff678a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Alternative to using parentheses**: Use **backslash ( \\ )** at end of each of the multi-lines\n",
    "\n",
    "In this example, notice the parentheses at lines 9 and and 24 were substituted by backslash at end of each line of the multi-line transformation.\n",
    "\n",
    "*Tips*: \n",
    "* Ensure no-trailing spaces after each backslash ( \\ )\n",
    "* Backslash ( \\ ) doesn't allow in-line comments after the backslash ( \\ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f71b0d-26e3-424e-91bd-ec359c13c289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_lineitem_aliased1 = df_lineitem.alias(\"df_lineitem_aliased1\") \n",
    "df_partsupp_aliased1 = df_partsupp.alias(\"df_partsupp_aliased1\")\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "#To keep it brief, I’m showing a trimmed-down version of the code.\n",
    "#-----------------------------------------------------------------\n",
    "df_tips = \\\n",
    "    df_lineitem_aliased1 \\\n",
    "    .join(df_partsupp_aliased1, \\\n",
    "\n",
    "        #using dot notation \\\n",
    "        [(df_lineitem_aliased1.l_partkey == df_partsupp_aliased1.ps_partkey) & \\\n",
    "        (df_lineitem_aliased1.l_suppkey == df_partsupp_aliased1.ps_suppkey)], \\\n",
    "\n",
    "        \"inner\" \\\n",
    "   ) \\\n",
    "    .select( \\\n",
    "    df_lineitem_aliased1.l_orderkey.alias(\"orderkey\"), \\\n",
    "    df_lineitem_aliased1.l_partkey.alias(\"partkey_InLI\") \\\n",
    "    ) \\\n",
    "    .sort(col(\"orderkey\").asc()) \\\n",
    "\n",
    "\n",
    "df_tips.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d65a6eeb-84cc-498a-a1d5-42e387fa51df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 3: If join condition is complex, consider defining it before performing the join\n",
    "\n",
    "In the below example, pretending that the join condition is complex:\n",
    "1. Notice how the join condition was defined and assigned to a variable (as in lines# 7-10), before performing the join\n",
    "2. And, join transformation (as in line# 16) is referencing the variable representing the join condition\n",
    "\n",
    "What's the benefit?\n",
    "* Improved code readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977e549e-dd7b-4a7e-a7c2-52a66895e9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_lineitem_aliased1 = df_lineitem.alias(\"df_lineitem_aliased1\") \n",
    "df_partsupp_aliased1 = df_partsupp.alias(\"df_partsupp_aliased1\")\n",
    "\n",
    "#pretending that the join condition is complex, define the join condition and store it in a list variable\n",
    "join_condition_list_1 = [\n",
    "    (df_lineitem_aliased1.l_partkey == df_partsupp_aliased1.ps_partkey) &\n",
    "    (df_lineitem_aliased1.l_suppkey == df_partsupp_aliased1.ps_suppkey)\n",
    "]\n",
    "\n",
    "df_tips = (\n",
    "    df_lineitem_aliased1\n",
    "    .join(df_partsupp_aliased1, \n",
    "\n",
    "        join_condition_list_1,\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "    df_lineitem_aliased1.l_orderkey.alias(\"orderkey\"),\n",
    "    df_partsupp_aliased1.ps_partkey.alias(\"partkey_inPartSupp\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_tips.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e1f5f2d-1b27-4da6-92ac-19fd67a3bbb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 4: In a multi-column join, enclose each column join expression in a separate parentheses ()\n",
    "\n",
    "While specifying join condition on multiple columns, \n",
    "* enclose each column join expression in a separate parentheses ()\n",
    "  - e.g. `(df_lineitem_aliased1.l_partkey == df_partsupp_aliased1.ps_partkey)`\n",
    "* optionally, enclose all column join expressions in a List[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c703a4cf-420f-44bd-97ee-e9f0bb51882c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "#To keep it brief, I’m showing a trimmed-down version of the code.\n",
    "#-----------------------------------------------------------------\n",
    "'''\n",
    "df_tips = (\n",
    "    df_lineitem_aliased1\n",
    "    .join(df_partsupp_aliased1, \n",
    "\n",
    "        [\n",
    "            (df_lineitem_aliased1.l_partkey == df_partsupp_aliased1.ps_partkey) &\n",
    "            (df_lineitem_aliased1.l_suppkey == df_partsupp_aliased1.ps_suppkey)\n",
    "        ],\n",
    "\n",
    "        \"inner\"\n",
    "   )\n",
    "    #...\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a234da67-97c7-4193-8757-1322b417acc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 5: In a multi-line transformation, \".select()\"  statement can start either on the same line as the preceding \")\" or on the next line. \n",
    "\n",
    "When you use parentheses to wrap multi-line transformation statement, you could start the `.select()` statement either on the same line as the previous `)` or on the next line.\n",
    "\n",
    "For example, as shown in the trimmed-down version of the code below, the `.select()` could be started either on line# 10 itself or on the next line (line# 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a9b5d9-9f28-4969-a2ee-81b5eec342ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "#To keep it brief, I’m showing a trimmed-down version of the code.\n",
    "#-----------------------------------------------------------------\n",
    "'''\n",
    "df_tips = (\n",
    "    df_lineitem_aliased1\n",
    "    .join(df_partsupp_aliased1, \n",
    "        ...,\n",
    "        \"inner\"\n",
    "   )#.select( #Tip: You could write .select() in this line itself or in the next line\n",
    "    .select(\n",
    "        ...\n",
    "    )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f004c96-8d5a-41ba-b78b-494f1e517cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 6: Avoid using select('*'); instead, select only the columns you actually need.\n",
    "\n",
    "As a best practice, in a `select()` statement in general and as part of a `join()` transformation in particular, avoid using `select('*')`. Instead, select only those columns that are necessary.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "A `join()` transformation is a wide transformation, which means it triggers a **shuffle** (a.k.a exchange). During a shuffle, Spark redistributes data across the cluster based on the join key and writes intermediate results to disk. This process is expensive in terms of both disk I/O and network transfer.\n",
    "\n",
    "While shuffles are often unavoidable when performing joins, we can still optimize performance by minimizing the columns selected in the output — that is, avoid including redundant or unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93922ab5-3c8c-43f7-99b2-a6aa982a83ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "#To keep it brief, I’m showing a trimmed-down version of the code.\n",
    "#-----------------------------------------------------------------\n",
    "'''\n",
    "df_tips = (\n",
    "    #....\n",
    "    .select(\n",
    "    df_lineitem_aliased1.l_orderkey.alias(\"orderkey\"),\n",
    "    df_lineitem_aliased1.l_partkey.alias(\"partkey_InLI\"),\n",
    "    df_lineitem_aliased1.l_suppkey.alias(\"suppkey_InLI\"),\n",
    "    df_lineitem_aliased1.l_linenumber.alias(\"linenumber\"),\n",
    "    df_lineitem_aliased1.l_quantity.alias(\"quantity\"),\n",
    "    df_partsupp_aliased1.ps_partkey.alias(\"partkey_inPartSupp\"),\n",
    "    df_partsupp_aliased1.ps_suppkey.alias(\"suppkey_inPartSupp\"),\n",
    "    df_partsupp_aliased1.ps_availqty.alias(\"availqty\")\n",
    "    )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d3f62d-b1e7-4363-af40-60ab3b2b770f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 7: Alias columns to avoid ambiguous column names in output of transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68b24c75-700f-4a52-8eb9-b15243b91164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem scenario:**\n",
    "\n",
    "What if an identical column name appeared in both the dataframes being joined?\n",
    "\n",
    "For example, we want to join Customer table with Nation table. But, what if both Customer table and Nation table have a column by name 'NAME'. And, we want the output to contain both the NAME columns?\n",
    "\n",
    "**Solution:** Alias column names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe60b652-ba51-490e-bab1-8a28a90f1d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Example data for this demo:**\n",
    "\n",
    "Customer dataframe:\n",
    "\n",
    "| cust_id | NAME | nation_id |\n",
    "|--------|----------|------------|\n",
    "|101|David|1|\n",
    "|102|Kevin|2|\n",
    "|103|Lisa|2|\n",
    "\n",
    "Nation dataframe:\n",
    "\n",
    "| nation_id | NAME |\n",
    "|--------|----------|\n",
    "|1|Australia|\n",
    "|2|India|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85291588-ca96-4faa-98aa-f7434235147b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. load customer dataframe\n",
    "customer_data = (\n",
    "    (101,\"David\",1),\n",
    "    (102,\"Kevin\",2),\n",
    "    (103,\"Lisa\",2)\n",
    "  )\n",
    "customer_schema = \"cust_id: int, NAME: string, nation_id: int\"\n",
    "\n",
    "df_customer_tip7 = spark.createDataFrame(data = customer_data, schema = customer_schema)\n",
    "\n",
    "#2. load nation dataframe\n",
    "nation_data = (\n",
    "    (1,\"Australia\"),\n",
    "    (2,\"India\")\n",
    "  )\n",
    "nation_schema = \"nation_id: int, NAME: string\"\n",
    "\n",
    "df_nation_tip7 = spark.createDataFrame(data = nation_data, schema = nation_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcd04bdf-5e54-4b24-9862-f3d161ecd54a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Scenario 1**: Alias output column names in a transformation to avoid ambiguity in column names\n",
    "\n",
    "The printSchema() output looks as below:\n",
    "\n",
    "```\n",
    " |-- Employee_Id: integer (nullable = true)\n",
    " |-- Employee_Name: string (nullable = true)\n",
    " |-- Employee_Job_Title: string (nullable = true)\n",
    " |-- Manager_Id: integer (nullable = true)\n",
    " |-- Manager_Name: string (nullable = true)\n",
    " |-- Manager_Job_Title: string (nullable = true)\n",
    "```\n",
    "Notice in the printSchema() ouput duplicate column names have been eliminated due to column aliasing appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d0a3313-9217-4cfa-a829-d197940b2c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "df_cust = df_customer_tip7.alias(\"df_cust\")\n",
    "df_nat = df_nation_tip7.alias(\"df_nat\")\n",
    "\n",
    "df_tip7 = (\n",
    "    df_cust\n",
    "    .join(\n",
    "        df_nat,\n",
    "        df_cust.nation_id == df_nat.nation_id,\n",
    "        \"left_outer\" \n",
    "    )\n",
    "    .select(\n",
    "        #Notice how the column names are aliased, especially the identical column names from both the dataframes\n",
    "        df_cust.cust_id.alias(\"Customer_Id\"),\n",
    "        df_cust.NAME.alias(\"Customer_Name\"),\n",
    "        df_nat.NAME.alias(\"Customer_Country\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_tip7.printSchema()\n",
    "\n",
    "df_tip7.select(\"*\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c8701a3-7096-4bbc-8517-fd57d3676947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 8: Avoid sort()/orderBy() unless it is necessary\n",
    "\n",
    "As a best practice, avoid `sort()` / `orderBy()` unless you actually need it.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Similar to `join()` transformation, even `sort()` / `orderBy()` transformation is a wide transformation, which means it triggers a **shuffle** (a.k.a exchange). During a shuffle, Spark redistributes data across the cluster based on the join key and writes intermediate results to disk. This process is expensive in terms of both disk I/O and network transfer.\n",
    "\n",
    "So, avoid sorting data in a transformation unless you really need it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e2d097-52f9-4cd8-94a3-b3eb83335398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "#To keep it brief, I’m showing a trimmed-down version of the code.\n",
    "#-----------------------------------------------------------------\n",
    "'''\n",
    "df_tips = (\n",
    "    df_lineitem_aliased1\n",
    "    .join(df_partsupp_aliased1, \n",
    "        ...\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "    df_lineitem_aliased1.l_orderkey.alias(\"orderkey\"),\n",
    "    df_lineitem_aliased1.l_partkey.alias(\"partkey_InLI\")\n",
    "    )\n",
    "    #.sort(col(\"orderkey\").asc())\n",
    "    .orderBy(col(\"orderkey\").asc())\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4fe2b5d-81a0-4a1a-9751-90865fed744b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 9: For code brevity, prefer col() to reference aliased column names in sort() / orderBy(). \n",
    "\n",
    "Focusing on line# 16-19 in trimmed-down version of the code as below: \n",
    "\n",
    "To `sort()` the output, Two different approaches were shown:\n",
    "* Line# 16 - using `col()`\n",
    "* Line# 19 - using dot notation\n",
    "\n",
    "Between the two approaches, I think `col()` approach is concise, on this occasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c093c6ad-be14-4a66-b964-830f091d41d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "#To keep it brief, I’m showing a trimmed-down version of the code.\n",
    "#-----------------------------------------------------------------\n",
    "'''\n",
    "df_tips = (\n",
    "    df_lineitem_aliased1\n",
    "    .join(df_partsupp_aliased1, \n",
    "        [(...) & (...)],\n",
    "        \"inner\"\n",
    "   )\n",
    "    .select(\n",
    "        ...\n",
    "    )\n",
    "\n",
    "    #For code brevity, prefer referencing aliased column names in sort() by using col()\n",
    "    .sort(col(\"orderkey\").asc(), col(\"linenumber\").asc()) \n",
    "\n",
    "    #To use dot notation in sort(), you'll need to reference original column name rather than aliased name\n",
    "    #.sort(df_lineitem_aliased1.l_orderkey.asc(), df_lineitem_aliased1.l_linenumber.asc()) \n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77cf6475-d598-4c3a-8bec-5d0a91cd078e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 10: Remove display() / show() function calls before promoting code to higher environments.\n",
    "\n",
    "e.g. `df_tips.display()`\n",
    "\n",
    "`display()` and `show()` are actions. Unlike transformations, Actions force the computation to occur.\n",
    "\n",
    "So, to avoid unnecessary compute costs, make sure to remove those lines of code that call `display()` / `show()` functions before promoting the code to higher environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d18a07c8-d1e9-44c2-b7cd-80b4f64842e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tip 11: Want to convert SQL query into PySpark query? AI Assistant could help!\n",
    "\n",
    "Say, you got a SQL query and for what so ever reason, you want to convert it into PySpark...How can we go about doing it?\n",
    "\n",
    "Two options:\n",
    "1. Write PySpark query from the scratch\n",
    "2. Or, get AI Assistant in Databricks notebook to do the initial conversion of SQL query into PySpark query. Then, refine the initial version of PySpark query to meet your exact needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df859dd-ae1a-43ad-9aa9-9d1eca9422ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  li.l_orderkey AS orderkey,\n",
    "  li.l_partkey AS partkey_InLI,\n",
    "  li.l_suppkey AS suppkey_InLI,\n",
    "  li.l_linenumber AS linenumber,\n",
    "  li.l_quantity AS quantity,\n",
    "  ps.ps_partkey AS partkey_inPartSupp,\n",
    "  ps.ps_suppkey AS suppkey_inPartSupp,\n",
    "  ps.ps_availqty AS availqty,\n",
    "  ps.ps_supplycost AS supplycost,\n",
    "  p.p_partkey AS partkey_inPart,\n",
    "  p.p_name AS partname,\n",
    "  s.s_suppkey AS suppkey_inSupplier,\n",
    "  s.s_name AS suppliername\n",
    "FROM samples.tpch.lineitem li\n",
    "INNER JOIN samples.tpch.partsupp ps\n",
    "  ON li.l_partkey = ps.ps_partkey AND li.l_suppkey = ps.ps_suppkey\n",
    "INNER JOIN samples.tpch.part p\n",
    "  ON ps.ps_partkey = p.p_partkey\n",
    "INNER JOIN samples.tpch.supplier s\n",
    "  ON ps.ps_suppkey = s.s_suppkey\n",
    "ORDER BY orderkey ASC, linenumber ASC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b57238b2-40b3-40b9-be29-38e413087544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query as converted by AI Assistant into PySpark query looks as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8454c1ef-7992-4b4c-be4f-09b0ebdd8b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark equivalent of the SQL query (as in the preceding cell), as converted by AI Assistant in Databricks notebook.\n",
    "\n",
    "li = spark.table(\"samples.tpch.lineitem\").alias(\"li\")\n",
    "ps = spark.table(\"samples.tpch.partsupp\").alias(\"ps\")\n",
    "p = spark.table(\"samples.tpch.part\").alias(\"p\")\n",
    "s = spark.table(\"samples.tpch.supplier\").alias(\"s\")\n",
    "\n",
    "result = (\n",
    "    li.join(ps, (li.l_partkey == ps.ps_partkey) & (li.l_suppkey == ps.ps_suppkey))\n",
    "      .join(p, ps.ps_partkey == p.p_partkey)\n",
    "      .join(s, ps.ps_suppkey == s.s_suppkey)\n",
    "      .select(\n",
    "          li.l_orderkey.alias(\"orderkey\"),\n",
    "          li.l_partkey.alias(\"partkey_InLI\"),\n",
    "          li.l_suppkey.alias(\"suppkey_InLI\"),\n",
    "          li.l_linenumber.alias(\"linenumber\"),\n",
    "          li.l_quantity.alias(\"quantity\"),\n",
    "          ps.ps_partkey.alias(\"partkey_inPartSupp\"),\n",
    "          ps.ps_suppkey.alias(\"suppkey_inPartSupp\"),\n",
    "          ps.ps_availqty.alias(\"availqty\"),\n",
    "          ps.ps_supplycost.alias(\"supplycost\"),\n",
    "          p.p_partkey.alias(\"partkey_inPart\"),\n",
    "          p.p_name.alias(\"partname\"),\n",
    "          s.s_suppkey.alias(\"suppkey_inSupplier\"),\n",
    "          s.s_name.alias(\"suppliername\")\n",
    "      )\n",
    "      .orderBy(\"orderkey\", \"linenumber\")\n",
    ")\n",
    "\n",
    "# To display the result in Databricks notebook\n",
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5628658042467098,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Joins - Part 2 - Beyond the Basics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
